[
{"content":{"body":"i apologize for lack of communication for past several days, i'm quite behind on some stuff","msgtype":"m.text"},"ts":1614653611000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"wednesday is dedicated tc39 day","msgtype":"m.text"},"ts":1614653617000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"parts of the temporal spec are wild","msgtype":"m.text"},"ts":1614721172000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"how is it pulling bigints out of thin air","msgtype":"m.text"},"ts":1614721179000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"aww, the multipage build failed to upload: https://travis-ci.org/github/tc39/ecma262/jobs/761193804","msgtype":"m.text"},"ts":1614723611000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"Error: POST failed with: 413, body: { message: 'Request Too Long' }","msgtype":"m.text"},"ts":1614723640000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"ljharb: i saw you have a call to discuss \"from\" lookups in temporal tomorrow","msgtype":"m.text"},"ts":1614723898000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"i have a conflict for the first 30 minutes of that meeting, but my position is: it is very weird to me","msgtype":"m.text"},"ts":1614723911000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"especially because it is a lookup off %Temporal.Calendar%, not Temporal.Calendar","msgtype":"m.text"},"ts":1614723927000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"i'd like to hear the justification for a method to be a possible extension point, but not the objects that contain it","msgtype":"m.text"},"ts":1614723967000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"i will be on that call as well; I had the same concern","msgtype":"m.text"},"ts":1614724084000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"thanks, please relay mine if i can't make it","msgtype":"m.text"},"ts":1614724141000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"but i'll try to show up on the second half","msgtype":"m.text"},"ts":1614724145000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"in general there are too many hook points for my liking","msgtype":"m.text"},"ts":1614724567000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"somehow it both has @@species **and** a per-method hook point, and then also asserts that the returned value from possibly monkey-patched methods also have an expected internal slot","msgtype":"m.text"},"ts":1614724804000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"it is very hard for me to wrap my head around","msgtype":"m.text"},"ts":1614724813000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"bakkot: ah right, thereâ€™s a hard size limit due to aws","msgtype":"m.text"},"ts":1614725258000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"and yeah their idea of extensibility is just bizarre","msgtype":"m.text"},"ts":1614725276000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"we simply will not be adding anything that encourages modifying builtins as an extension mechanism","msgtype":"m.text"},"ts":1614725298000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"i also find that the Calendar methods accept string|Object to be problematic","msgtype":"m.text"},"ts":1614726405000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"that's where a lot of the problems come from actually","msgtype":"m.text"},"ts":1614726434000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"not really an editorial opinion, but i'd like calls into any kind of parser to be nice and explicit","msgtype":"m.text"},"ts":1614726434000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"basically i'm wary to even finish reviewing this spec draft","msgtype":"m.text"},"ts":1614726460000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"because i am Deeply Afraid of this being a security bug farm","msgtype":"m.text"},"ts":1614726468000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"the decision to make a bunch of APIs consume strings lead to the decision to have a central place where you can define new calendars to be used by those APIs","msgtype":"m.text"},"ts":1614726492000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"solution is, I would think, do not add a bunch of APIs which take strings","msgtype":"m.text"},"ts":1614726513000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"yes","msgtype":"m.text"},"ts":1614726516000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"i agree","msgtype":"m.text"},"ts":1614726520000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"i... kinda feel bad that this is a pretty fundamental objection and they've been working on this for so long","msgtype":"m.text"},"ts":1614726539000,"senderName":"shu","senderId":"shu@irc"},
{"content":{"body":"my hope is that it won't prove to be that fundamental","msgtype":"m.text"},"ts":1614726588000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"we'll see though","msgtype":"m.text"},"ts":1614726590000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"fwiw I think they do actually manage to have ~ a single place which calls out to the parser, which is in the .from methods","msgtype":"m.text"},"ts":1614726660000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"all the string-consuming APIs look up and call something.from","msgtype":"m.text"},"ts":1614726672000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"I think","msgtype":"m.text"},"ts":1614726672000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"i believe that' strue","msgtype":"m.text"},"ts":1614726710000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"ljharb re: size limit, do you know what the limit is and/or how to work around it?","msgtype":"m.text"},"ts":1614726745000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"it's small, like a meg or two, and there's no way to work around it i'm aware of - i'm told it's a fundamental limitation to AWS payloads","msgtype":"m.text"},"ts":1614726780000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"I'm guessing it's 5 megs","msgtype":"m.text"},"ts":1614726791000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"yeah that sounds right","msgtype":"m.text"},"ts":1614726794000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"i suppose we could upload to S3 and then serve from there, but then we'd have to pay for our own S3 storage","msgtype":"m.text"},"ts":1614726800000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"since the payload is 5005.249kB","msgtype":"m.text"},"ts":1614726801000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"are things like images duplicated?","msgtype":"m.text"},"ts":1614726811000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"no, only duplicated thing is the table of contents","msgtype":"m.text"},"ts":1614726819000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"which tbf is massive","msgtype":"m.text"},"ts":1614726822000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"is it possible to upload a tarball or something, and have begin untar it?","msgtype":"m.text"},"ts":1614726849000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"ah, hm","msgtype":"m.text"},"ts":1614726850000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"p sure that's what we do","msgtype":"m.text"},"ts":1614726857000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"altho maybe not, i'll have to look into it","msgtype":"m.text"},"ts":1614726870000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"i think that the issue is that amazon won't untar it","msgtype":"m.text"},"ts":1614726879000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"and the limit is based on what untarred contents you can deploy to aws","msgtype":"m.text"},"ts":1614726893000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"oh, we do gzip it, looks like","msgtype":"m.text"},"ts":1614726900000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"hah: https://github.com/tc39/ecma262/blob/master/scripts/publish-preview.js#L43","msgtype":"m.text"},"ts":1614726907000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"apparently the script has a check which is slightly larger than the actual size","msgtype":"m.text"},"ts":1614726922000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"that sounds right","msgtype":"m.text"},"ts":1614726930000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"*actual limit","msgtype":"m.text"},"ts":1614726932000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"the serverside is https://github.com/ljharb/tc39-ci/tree/master/src/http/post-preview-000u-000r","msgtype":"m.text"},"ts":1614726977000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"so yeah it looks like it's stored on s3, unzipped","msgtype":"m.text"},"ts":1614727000000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"that looks like it's uploading individual files, so the full bundle shouldn't be a problem","msgtype":"m.text"},"ts":1614727130000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"oh, and the compression is also per-file, rather than the whole thing","msgtype":"m.text"},"ts":1614727212000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"is the serverside thing something we control?","msgtype":"m.text"},"ts":1614727220000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"'cause making it a gzip-of-list-of-files rather than a list-of-gzip-of-files would probably be sufficient","msgtype":"m.text"},"ts":1614727246000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"the server endpoint certainly is, but i don't think that's compatible with s3","msgtype":"m.text"},"ts":1614727473000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"meaning i think it's a list of gzips on purpose - you can't upload a zip of files to S3 and have it extracted, you have to upload each file individually","msgtype":"m.text"},"ts":1614727497000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"uh","msgtype":"m.text"},"ts":1614727534000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"maybe I am confused","msgtype":"m.text"},"ts":1614727535000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"where does https://github.com/ljharb/tc39-ci/blob/master/src/http/post-preview-000u-000r/index.js run?","msgtype":"m.text"},"ts":1614727539000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"in begin's aws lambda, i believe","msgtype":"m.text"},"ts":1614727573000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"from reading that file it looks like there's two phases: files are sent to this script, and this script puts them, one at a time, into S3","msgtype":"m.text"},"ts":1614727586000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"right","msgtype":"m.text"},"ts":1614727589000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"and i believe that's the only way S3 accepts files","msgtype":"m.text"},"ts":1614727597000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"sure","msgtype":"m.text"},"ts":1614727601000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"so, this script runs code","msgtype":"m.text"},"ts":1614727609000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"it could therefore do the unzipping itself","msgtype":"m.text"},"ts":1614727620000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"and the size limit is all about what files are downloaded onto the lambda for https://github.com/ljharb/tc39-ci/tree/master/src/http/get-preview-000u-000r-000k-000etc to use","msgtype":"m.text"},"ts":1614727629000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"oh so you're just talking about between CI, and the POST endpoint","msgtype":"m.text"},"ts":1614727643000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"that we can certainly change","msgtype":"m.text"},"ts":1614727646000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"but i'm not sure that helps the root of the problem","msgtype":"m.text"},"ts":1614727659000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"I think it must be the root of the problem: the files are uploaded to S3 individually","msgtype":"m.text"},"ts":1614727674000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"and each file is small","msgtype":"m.text"},"ts":1614727688000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"the only individual message which is large is the one between CI and this endpoint","msgtype":"m.text"},"ts":1614727706000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"right but the size limit is \"the sum total of the unzipped files\"","msgtype":"m.text"},"ts":1614727712000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"\"the unzipped size on disk\", iow","msgtype":"m.text"},"ts":1614727723000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"why unzipped?","msgtype":"m.text"},"ts":1614727727000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"because that's the way S3 is architected to require.","msgtype":"m.text"},"ts":1614727742000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"and lambda","msgtype":"m.text"},"ts":1614727748000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"no, I mean, our s3 clearly has much more than 5mb of files on it","msgtype":"m.text"},"ts":1614727759000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"since it has a whole bunch of previews","msgtype":"m.text"},"ts":1614727764000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"yeah it's not the s3, it's the lambda","msgtype":"m.text"},"ts":1614727767000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"https://github.com/ljharb/tc39-ci/blob/master/src/http/get-preview-000u-000r-000k-000etc/index.js","msgtype":"m.text"},"ts":1614727769000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"each one is only allocated 6MB of disk space for the entire process","msgtype":"m.text"},"ts":1614727782000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"(is my understanding)","msgtype":"m.text"},"ts":1614727784000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"oh, sure, but you solve that by not unzipping the whole thing at once","msgtype":"m.text"},"ts":1614727794000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"but it all has to be unzipped to be able to serve it later","msgtype":"m.text"},"ts":1614727809000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"but not... on the lambda, right?","msgtype":"m.text"},"ts":1614727841000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"like the files live in s3","msgtype":"m.text"},"ts":1614727843000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"yes, on the lambda","msgtype":"m.text"},"ts":1614727845000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"no","msgtype":"m.text"},"ts":1614727845000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"they live in s3 but are copied to the lambda to be able to be served","msgtype":"m.text"},"ts":1614727854000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"actually that doesn't sound right","msgtype":"m.text"},"ts":1614727885000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"... are they?","msgtype":"m.text"},"ts":1614727887000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"lol i pushed back on this really hard when setting it all up, and was told that it's an immutable constraint","msgtype":"m.text"},"ts":1614727905000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"i'll ask again to try to recall why","msgtype":"m.text"},"ts":1614727910000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"see if you can get them to be more specific about what the constraint actually is","msgtype":"m.text"},"ts":1614727937000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"i.e. is it \"the size of the message received by the POST endpoint\" or \"the amount of RAM used by the endpoint during upload\" or what","msgtype":"m.text"},"ts":1614727961000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"i suspect it's the former, but i'll ask and get back to you","msgtype":"m.text"},"ts":1614728020000,"senderName":"ljharb","senderId":"ljharb@irc"},
{"content":{"body":"many thanks","msgtype":"m.text"},"ts":1614728058000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"if it's the former we can fix it by switching from list-of-gzip-of-files to gzip-of-list-of-files","msgtype":"m.text"},"ts":1614728110000,"senderName":"bakkot","senderId":"bakkot@irc"},
{"content":{"body":"fix it for this particular case, specifically, because in this particular case the gzip-of-list-of-files will compress much better.","msgtype":"m.text"},"ts":1614728141000,"senderName":"bakkot","senderId":"bakkot@irc"}
]