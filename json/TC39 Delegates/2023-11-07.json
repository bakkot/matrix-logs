[
{"content":{"body":"openai announced a new Whisper model, but still no streaming support :(\n\nall the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.\n\nwhisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).\n\nI don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)\n\nI might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.\n\nanyone happen to have played with any other promising real-time transcription services recently?","format":"org.matrix.custom.html","formatted_body":"<p>openai announced a new Whisper model, but still no streaming support :(</p>\n<p>all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.</p>\n<p>whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).</p>\n<p>I don't think automatic transcription is going to be viable until something in this landscape changes. (cc <a href=\"https://matrix.to/#/@littledan:matrix.org\">littledan</a>)</p>\n<p>I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.</p>\n<p>anyone happen to have played with any other promising real-time transcription services recently?</p>\n","m.mentions":{"user_ids":["@littledan:matrix.org"]},"msgtype":"m.text"},"ts":1699340695799,"senderName":"bakkot","senderId":"@bakkot:matrix.org","id":"$V_xVF4NC-XHRMqvTyLitVcTPosO9ZH0m4JxrWMcvthk"},
{"content":{"body":"> <@bakkot:matrix.org> openai announced a new Whisper model, but still no streaming support :(\n> \n> all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.\n> \n> whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).\n> \n> I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)\n> \n> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.\n> \n> anyone happen to have played with any other promising real-time transcription services recently?\n\nsaminahusain: ","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!WgJwmjBNZEXhJnXHXw:matrix.org/$V_xVF4NC-XHRMqvTyLitVcTPosO9ZH0m4JxrWMcvthk?via=matrix.org&via=igalia.com&via=mozilla.org\">In reply to</a> <a href=\"https://matrix.to/#/@bakkot:matrix.org\">@bakkot:matrix.org</a><br><p>openai announced a new Whisper model, but still no streaming support :(</p>\n<p>all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.</p>\n<p>whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).</p>\n<p>I don't think automatic transcription is going to be viable until something in this landscape changes. (cc <a href=\"https://matrix.to/#/@littledan:matrix.org\">littledan</a>)</p>\n<p>I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.</p>\n<p>anyone happen to have played with any other promising real-time transcription services recently?</p>\n</blockquote></mx-reply><a href=\"https://matrix.to/#/@saminahusain:matrix.org\">saminahusain</a>:","m.mentions":{"user_ids":["@bakkot:matrix.org","@saminahusain:matrix.org"]},"m.relates_to":{"m.in_reply_to":{"event_id":"$V_xVF4NC-XHRMqvTyLitVcTPosO9ZH0m4JxrWMcvthk"}},"msgtype":"m.text"},"ts":1699355017883,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$QkVeR8XZjLyRgFeTr0NxEh_PxQhdKXJhE81CwZMwbD0"},
{"content":{"body":"Thanks for the report, bakkot . Let's check up on this again at the end of next year.","format":"org.matrix.custom.html","formatted_body":"Thanks for the report, <a href=\"https://matrix.to/#/@bakkot:matrix.org\">bakkot</a> . Let's check up on this again at the end of next year.","m.mentions":{"user_ids":["@bakkot:matrix.org"]},"msgtype":"m.text"},"ts":1699355044429,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$jTvToOH1O8tz1kRTfBYSdd-4B11ZemFBOOby-kvAcPE"},
{"content":{"body":"sounds like we need to repeat the budget request for transcriptionists","m.mentions":{},"msgtype":"m.text"},"ts":1699355056687,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$ykToEfOD1NsHsHKrCJka3mgNIwKYPJc1IYzbZNQyrdk"},
{"content":{"body":"Are you saying we get good accuracy with a 40-second delay?","m.mentions":{},"msgtype":"m.text"},"ts":1699355073931,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$zfRcFPNBgh-Tq4o9MsKZw_33jT02r7qJu2MmOg7uzw0"},
{"content":{"body":"it would be accurate, yeah IIUC","m.mentions":{},"msgtype":"m.text"},"ts":1699355098007,"senderName":"ryzokuken","senderId":"@usharma:igalia.com","id":"$e7VIUQJ9qLxVu5LDcM8KmH-63DHIrhIA8hQakhZq3vw"},
{"content":{"body":"the 40 second delay is whisper's only shortcoming","m.mentions":{},"msgtype":"m.text"},"ts":1699355117188,"senderName":"ryzokuken","senderId":"@usharma:igalia.com","id":"$GA_JYlNzXybsdGUJPsWWLj4wCoHKsfkLyiJqbrCH1HM"},
{"content":{"body":"actually, I haven't tried it myself. Wonder how well it does with various accents","m.mentions":{},"msgtype":"m.text"},"ts":1699355192966,"senderName":"ryzokuken","senderId":"@usharma:igalia.com","id":"$6ZuXAnICKC4M4HpB3vFYyvgSkS0BS5VcXrs3wx_wccc"},
{"content":{"body":"they have an example with a pretty thick accent though, fun","m.mentions":{},"msgtype":"m.text"},"ts":1699355264331,"senderName":"ryzokuken","senderId":"@usharma:igalia.com","id":"$0SfNKuWvvT6Ifc26PENdeUcbVSoGNO_84ZIPPlEgr88"},
{"content":{"body":"https://openai.com/research/whisper","m.mentions":{},"msgtype":"m.text"},"ts":1699355265834,"senderName":"ryzokuken","senderId":"@usharma:igalia.com","id":"$Q7zTY6PTIQXfpKF5LLOAHyiSaImQymem9lIfIXLexOE"},
{"content":{"body":"right. Whisper is very accurate in my tests, but it fundamentally operates on 30-second chunks of audio and takes a little while to run (say 10 seconds per chunk), so trying to stream it to the notes doc would mean that every 30 seconds we get a high-quality transcript of the portion of the meeting starting 40 seconds ago and running through 10 seconds ago. I haven't actually set that up but I expect it to work.","m.mentions":{},"msgtype":"m.text"},"ts":1699369914820,"senderName":"bakkot","senderId":"@bakkot:matrix.org","id":"$VakAyOj6KJK-VCaWMzphsW0WCw83L7I5Hy4mcSgFLzM"},
{"content":{"body":"unfortunately 40 seconds of lag is a lot of lag","m.mentions":{},"msgtype":"m.text"},"ts":1699369939097,"senderName":"bakkot","senderId":"@bakkot:matrix.org","id":"$rlq5ZHA1IWyr5WJXujVed6WaoiBU3YCQhSFYhJAFF68"},
{"content":{"body":"> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.\n\nThat would be *so* helpful!","format":"org.matrix.custom.html","formatted_body":"<blockquote>\n<p>I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.</p>\n</blockquote>\n<p>That would be <em>so</em> helpful!</p>\n","m.mentions":{},"msgtype":"m.text"},"ts":1699370711927,"senderName":"Michael Ficarra","senderId":"@michaelficarra:matrix.org","id":"$WHzYakj_YbxeaYYQcE5VV5sTnOLbjCGhd7liccVPfyA"}
]