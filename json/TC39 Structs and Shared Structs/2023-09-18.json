[
{"content":{"body":"> <@rbuckton:matrix.org> I'm still tinkering with my parallel parse prototype, and I'm planning to try it on a few large scale projects. I'm not currently seeing the perf-gains I would hope, but its too early to say if its an issue with the shared structs functionality, the size of the projects I've been using for testing, or something about how I've had to hack around parts of the compiler to get something functional.\n> I wrote a rudimentary work-stealing thread pooling mechanism, but I'm finding that adding more threads slows down parse rather than speeding it up for the monorepo I've been using as a test case. CPU profiling shows a lot of the threads aren't processing work efficiently, and are either spinning around trying to steal work or are waiting to be notified of work. Spinning isn't very efficient because there's no spin-wait mechanism nor the ability to write an efficient one (I can sort-of approximate one using `Condition.wait` with a short timeout to emulate `sleep`, but I can't efficiently yield). I also can't write efficient lock-free algorithms with shared structs alone, since I can't do CAS, so the fastest \"lock-free\"-ish updates I can perform are inside of a `Mutex.tryLock` unless I want to fall back to also sending a `SharedArrayBuffer` to the worker just so I can use `Atomics.compareExchange`.\n> \n> Here's a rough approximation of the thread pool I'm using right now, if anyone has suggestions or feedback: https://gist.github.com/rbuckton/3648f878595ed4e2ff3d52a15baaf6b9\n\nLooks good to me. Have you experimented with batch sizes? Each task being N files, rather than 1:1 task file ratio?","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk%3Amatrix.org/%24k0312Le1qqzBZ_AgI-7QIdIsuBN7__7Fv949n6o_JYs\">In reply to</a> <a href=\"https://matrix.to/#/@rbuckton:matrix.org\">@rbuckton:matrix.org</a><br><p>I'm still tinkering with my parallel parse prototype, and I'm planning to try it on a few large scale projects. I'm not currently seeing the perf-gains I would hope, but its too early to say if its an issue with the shared structs functionality, the size of the projects I've been using for testing, or something about how I've had to hack around parts of the compiler to get something functional.<br>I wrote a rudimentary work-stealing thread pooling mechanism, but I'm finding that adding more threads slows down parse rather than speeding it up for the monorepo I've been using as a test case. CPU profiling shows a lot of the threads aren't processing work efficiently, and are either spinning around trying to steal work or are waiting to be notified of work. Spinning isn't very efficient because there's no spin-wait mechanism nor the ability to write an efficient one (I can sort-of approximate one using <code>Condition.wait</code> with a short timeout to emulate <code>sleep</code>, but I can't efficiently yield). I also can't write efficient lock-free algorithms with shared structs alone, since I can't do CAS, so the fastest \"lock-free\"-ish updates I can perform are inside of a <code>Mutex.tryLock</code> unless I want to fall back to also sending a <code>SharedArrayBuffer</code> to the worker just so I can use <code>Atomics.compareExchange</code>.</p>\n<p>Here's a rough approximation of the thread pool I'm using right now, if anyone has suggestions or feedback: https://gist.github.com/rbuckton/3648f878595ed4e2ff3d52a15baaf6b9</p>\n</blockquote></mx-reply>Looks good to me. Have you experimented with batch sizes? Each task being N files, rather than 1:1 task file ratio?","m.relates_to":{"m.in_reply_to":{"event_id":"$k0312Le1qqzBZ_AgI-7QIdIsuBN7__7Fv949n6o_JYs"}},"msgtype":"m.text"},"ts":1695034188663,"senderName":"Ashley Claymore","senderId":"@aclaymore:matrix.org","id":"$9M9vnCNsGPYBWpCAzVYNm_KkSZpqIjQJyMhQev5i_-w"},
{"content":{"body":"Also wondering how much the tasks are known up front (one main glob) vs discovered as imports are found. I.e how well the queue can stay pumped?","format":"org.matrix.custom.html","formatted_body":"Also wondering how much the tasks are known up front (one main glob) vs discovered as imports are found. I.e how well the queue can stay pumped?","msgtype":"m.text"},"ts":1695034256958,"senderName":"Ashley Claymore","senderId":"@aclaymore:matrix.org","id":"$UCcwcvuX_i10r8S-dgrZO2TOz-RXkPY_GH7ECIGp134"},
{"content":{"body":"Tasks are 1:1 per file. With work stealing, batching would be less efficient since you could have threads sitting idle. ","format":"org.matrix.custom.html","formatted_body":"Tasks are 1:1 per file. With work stealing, batching would be less efficient since you could have threads sitting idle.","msgtype":"m.text"},"ts":1695034586704,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$iXwgVd6cmHGEs_cVQITsL1b07tkkx666eI4kq3IVlfQ"},
{"content":{"body":"How much is known upfront depends on the tsconfig `files`, `include`, and `exclude` options, though I'm using a striping approach to try to collect all imports/references for each pass around the file list. ","format":"org.matrix.custom.html","formatted_body":"How much is known upfront depends on the tsconfig <code>files</code>, <code>include</code>, and <code>exclude</code> options, though I'm using a striping approach to try to collect all imports/references for each pass around the file list.","msgtype":"m.text"},"ts":1695034728425,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$lQl_UEZJCUa5yrCQCLsljpTbvHjONBykWFPCAIIb5dA"},
{"content":{"body":"I need to experiment with a few more projects of different sizes though, it's still fairly early yet. ","format":"org.matrix.custom.html","formatted_body":"I need to experiment with a few more projects of different sizes though, it's still fairly early yet.","msgtype":"m.text"},"ts":1695034799871,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$FtXUEnpZkXJUMvLt_XF9AmJ4DTTArTJ6QDMAb1-fmKk"},
{"content":{"body":"The current approach is still very waterfall like in the main thread. I would need to do a lot more work to have the child threads scan for imports/references so they don't have to constantly wait for the main thread to hand out more work. ","format":"org.matrix.custom.html","formatted_body":"The current approach is still very waterfall like in the main thread. I would need to do a lot more work to have the child threads scan for imports/references so they don't have to constantly wait for the main thread to hand out more work.","msgtype":"m.text"},"ts":1695034953531,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$4CmcJ4fAhuwsT0fDnbpM0iCZZ0cdH02Fgm8980eGxS0"},
{"content":{"body":"Unfortunately, program.ts is very callback heavy and dependent on caches that would *also* need to be shared. ","format":"org.matrix.custom.html","formatted_body":"Unfortunately, program.ts is very callback heavy and dependent on caches that would <em>also</em> need to be shared.","msgtype":"m.text"},"ts":1695035046959,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$Hw1DHkC8eoOVjo6X8ENTcK1O4mKkJPqDxVZtQy-VTOA"},
{"content":{"body":"There's a lot of idle time waiting for main right now","msgtype":"m.text"},"ts":1695035102798,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$jo7jwZzAWVjh_a_cGrbCUmqDBn9OU5hnuGsbYuzHJto"},
{"content":{"body":"I currently have a synchronized, shareable `Map`-like data structure I can use for that, but I may want to see if I can build a lock-free, concurrent Map first so there's less blocking involved","format":"org.matrix.custom.html","formatted_body":"I currently have a synchronized, shareable <code>Map</code>-like data structure I can use for that, but I may want to see if I can build a lock-free, concurrent Map first so there's less blocking involved","msgtype":"m.text"},"ts":1695035284489,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$HaAXSR3d1YclXh-r2BbHPWn1kpVWuXphCAvJhVOEJrI"},
{"content":{"body":"> <@rbuckton:matrix.org> Tasks are 1:1 per file. With work stealing, batching would be less efficient since you could have threads sitting idle.\n\ntrue tho that assumes the queuing system is zero-cost (no padding around tasks). So might work out that some batching, while theoretically less efficient at packing, leads to better results.\nJust an idea :) ","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$iXwgVd6cmHGEs_cVQITsL1b07tkkx666eI4kq3IVlfQ?via=matrix.org&via=igalia.com&via=mozilla.org\">In reply to</a> <a href=\"https://matrix.to/#/@rbuckton:matrix.org\">@rbuckton:matrix.org</a><br>Tasks are 1:1 per file. With work stealing, batching would be less efficient since you could have threads sitting idle.</blockquote></mx-reply>true tho that assumes the queuing system is zero-cost (no padding around tasks). So might work out that some batching, while theoretically less efficient at packing, leads to better results.<br>Just an idea :)","m.relates_to":{"m.in_reply_to":{"event_id":"$iXwgVd6cmHGEs_cVQITsL1b07tkkx666eI4kq3IVlfQ"}},"msgtype":"m.text"},"ts":1695040012862,"senderName":"Ashley Claymore","senderId":"@aclaymore:matrix.org","id":"$ccgvBflUxHTJ5b5dl1V0DAxHx47iPITAxeDw7SYz_ZA"},
{"content":{"body":"In an ideal world parsing the largest files first would also be ideal for work stealing, though finding the largest files may be more costly than that saves too","msgtype":"m.text"},"ts":1695040096925,"senderName":"Ashley Claymore","senderId":"@aclaymore:matrix.org","id":"$HeD-1jqMrVVIQLsPiNKiaQQlQdiM4yQCMbYksWrPOBY"},
{"content":{"body":"is there slides of update?","msgtype":"m.text"},"ts":1695042547471,"senderName":"Jack Works","senderId":"@jackworks:matrix.org","id":"$Fzn8L3ezNnAhGxk_BU4iliWtzpN8vNcBJNoaGZLQKow"},
{"content":{"body":"I'm excited about the progress you've made and want to know more details! I can't wait!","msgtype":"m.text"},"ts":1695042581521,"senderName":"Jack Works","senderId":"@jackworks:matrix.org","id":"$TWkLd_Q_gWJ45vr6x23Mkdn9J8EeeuB1y99yMflddUQ"},
{"content":{"body":"Jack Works: there are in fact no slides yet :(","format":"org.matrix.custom.html","formatted_body":"<a href=\"https://matrix.to/#/@jackworks:matrix.org\">Jack Works</a>: there are in fact no slides yet :(","msgtype":"m.text"},"ts":1695048215324,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$Vhvaev2U5Rsb_Zn3fuvX1IZgYWNUTrAo4sGenO1fGTI"},
{"content":{"body":"got so much to do this week","msgtype":"m.text"},"ts":1695048219239,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$Pg_RLSUJazWglUKY614GduBDgAKC1C-kSiiJ5uxhS4M"},
{"content":{"body":"rbuckton: i wonder if also web workers sucking somehow is getting in the way of your performance? this is node though so who knows, might be unrelated to web workers even if its worker implementation were less than ideal","format":"org.matrix.custom.html","formatted_body":"<a href=\"https://matrix.to/#/@rbuckton:matrix.org\">rbuckton</a>: i wonder if also web workers sucking somehow is getting in the way of your performance? this is node though so who knows, might be unrelated to web workers even if its worker implementation were less than ideal","msgtype":"m.text"},"ts":1695048258193,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$1x6DMJpPaTlzpUFYUccvbemPJSDzDQWaO_LhgqiQnA4"},
{"content":{"body":"> <@aclaymore:matrix.org> true tho that assumes the queuing system is zero-cost (no padding around tasks). So might work out that some batching, while theoretically less efficient at packing, leads to better results.\n> Just an idea :)\n\nYou are possibly correct, though that is a level of fine tuning I'm not anywhere near investigating yet.","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$ccgvBflUxHTJ5b5dl1V0DAxHx47iPITAxeDw7SYz_ZA?via=matrix.org&via=igalia.com&via=mozilla.org\">In reply to</a> <a href=\"https://matrix.to/#/@aclaymore:matrix.org\">@aclaymore:matrix.org</a><br>true tho that assumes the queuing system is zero-cost (no padding around tasks). So might work out that some batching, while theoretically less efficient at packing, leads to better results.<br />Just an idea :)</blockquote></mx-reply>You are possibly correct, though that is a level of fine tuning I'm not anywhere near investigating yet.","m.relates_to":{"m.in_reply_to":{"event_id":"$ccgvBflUxHTJ5b5dl1V0DAxHx47iPITAxeDw7SYz_ZA"}},"msgtype":"m.text"},"ts":1695053007413,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$zevSt-sQzmpAZfRBldG4hPRWz2KdQKZtgrBwg7igQ20"},
{"content":{"body":"> <@shuyuguo:matrix.org> rbuckton: i wonder if also web workers sucking somehow is getting in the way of your performance? this is node though so who knows, might be unrelated to web workers even if its worker implementation were less than ideal\n\nAre you imagining there is overhead to reading/writing from shared structs or using mutex/condition caused by the worker? Or are you talking about overhead due as a result of setup, postMessage, etc.?","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$1x6DMJpPaTlzpUFYUccvbemPJSDzDQWaO_LhgqiQnA4?via=matrix.org&via=igalia.com&via=mozilla.org\">In reply to</a> <a href=\"https://matrix.to/#/@shuyuguo:matrix.org\">@shuyuguo:matrix.org</a><br><a href=\"https://matrix.to/#/@rbuckton:matrix.org\">rbuckton</a>: i wonder if also web workers sucking somehow is getting in the way of your performance? this is node though so who knows, might be unrelated to web workers even if its worker implementation were less than ideal</blockquote></mx-reply>Are you imagining there is overhead to reading/writing from shared structs or using mutex/condition caused by the worker? Or are you talking about overhead due as a result of setup, postMessage, etc.?","m.relates_to":{"m.in_reply_to":{"event_id":"$1x6DMJpPaTlzpUFYUccvbemPJSDzDQWaO_LhgqiQnA4"}},"msgtype":"m.text"},"ts":1695053089981,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$PeXhd3iFvgbRaFugIou2tc8K5_YGfls3vdKvOA2CZec"},
{"content":{"body":"I've updated the thread pool example to use a lock free Chase-Lev deque, though it still uses a Mutex/Condition to put the thread to sleep when there's no work to do.","msgtype":"m.text"},"ts":1695056915457,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$nM0t_9ikW97kliKOuq632fwC30LI6caYBDyTq5WG-_E"},
{"content":{"body":"It's still somewhat inefficient if a thread ends up sleeping and a task is added to a queue for a different thread that is still active.","msgtype":"m.text"},"ts":1695057960758,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$IN1bnnpzpQSPetvBWLXAsFAPGtO7H6FwW-1VFC54SH8"},
{"content":{"body":"Reading all this, I am still curious to understand how Shared Struct help compared to a synchronization mechanism (to implement a thread pool) coupled with an efficient message passing. How much actual shared mutable state is necessary?","msgtype":"m.text"},"ts":1695064565145,"senderName":"Mathieu Hofman","senderId":"@mhofman:matrix.org","id":"$cKsSdE1ktI02O2m9ChSMinLZIFcrxm2mNDIP8aBIGqg"},
{"content":{"body":"What would you consider to be \"efficient message passing\"? ","msgtype":"m.text"},"ts":1695066708726,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$dFH8Ub_URqh6R41kZ3dpFj_C-gob69JskpSB4j_NozY"},
{"content":{"body":"The lion's share of what TypeScript would send back and forth for parallel parse is essentially immutable, but a lot of the smaller data structures I need just to do coordination require shared mutable state.","msgtype":"m.text"},"ts":1695066793519,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$cbYHfmZT7I6ZFzRSQIYsqMoJjnp5O7Rd5jM5DwVQINI"},
{"content":{"body":"If I wanted to write my own `malloc`/`free` over a growable `SharedArrayBuffer` as a heap, I could mostly do the same things as what we can do with Shared Structs, albeit *far* slower due to the need for wrappers and indirection, plus I would have to handle string encoding/decoding on my own and could never shrink the size heap. Shared structs are far more efficient in this regard.","format":"org.matrix.custom.html","formatted_body":"If I wanted to write my own <code>malloc</code>/<code>free</code> over a growable <code>SharedArrayBuffer</code> as a heap, I could mostly do the same things as what we can do with Shared Structs, albeit <em>far</em> slower due to the need for wrappers and indirection, plus I would have to handle string encoding/decoding on my own and could never shrink the size heap. Shared structs are far more efficient in this regard.","msgtype":"m.text"},"ts":1695066907476,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$Rbo1InhnvvrENTjxb8wUi93jbMuBkLBSpYQmf3fjd1U"},
{"content":{"body":"And when I say \"could mostly do the same things\", I mean \"have done something very similar\" with https://esfx.js.org/esfx/api/struct-type.html, with the downside that it requires fixed sized types for fields and everything is laid out flat within a `SharedArrayBuffer`.","format":"org.matrix.custom.html","formatted_body":"And when I say \"could mostly do the same things\", I mean \"have done something very similar\" with https://esfx.js.org/esfx/api/struct-type.html, with the downside that it requires fixed sized types for fields and everything is laid out flat within a <code>SharedArrayBuffer</code>.","msgtype":"m.text"},"ts":1695067110453,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$neVzz1tEn2ZUS4ICMpohPbCBwbj6EgSg8y-kdK5-94Q"},
{"content":{"body":"(and it doesn't support arbitrary string values)","msgtype":"m.text"},"ts":1695067145587,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$ie043kcndtJjw1KYbKqOyPAPQN3TrE-1uHZMPyJL2Nc"},
{"content":{"body":"> <@rbuckton:matrix.org> Are you imagining there is overhead to reading/writing from shared structs or using mutex/condition caused by the worker? Or are you talking about overhead due as a result of setup, postMessage, etc.?\n\ni was thinking the latter, and scheduling","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$PeXhd3iFvgbRaFugIou2tc8K5_YGfls3vdKvOA2CZec?via=matrix.org&via=igalia.com&via=tchncs.de\">In reply to</a> <a href=\"https://matrix.to/#/@rbuckton:matrix.org\">@rbuckton:matrix.org</a><br>Are you imagining there is overhead to reading/writing from shared structs or using mutex/condition caused by the worker? Or are you talking about overhead due as a result of setup, postMessage, etc.?</blockquote></mx-reply>i was thinking the latter, and scheduling","m.relates_to":{"m.in_reply_to":{"event_id":"$PeXhd3iFvgbRaFugIou2tc8K5_YGfls3vdKvOA2CZec"}},"msgtype":"m.text"},"ts":1695068957123,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$nIhmTRfrA_tVzEqW1Pn2OCslDKvZZW7ZJoMY9GXe0f4"},
{"content":{"body":"> <@mhofman:matrix.org> Reading all this, I am still curious to understand how Shared Struct help compared to a synchronization mechanism (to implement a thread pool) coupled with an efficient message passing. How much actual shared mutable state is necessary?\n\nmy thinking has always been single-writer XOR multiple-reader kind of data sharing will get you pretty far","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$cKsSdE1ktI02O2m9ChSMinLZIFcrxm2mNDIP8aBIGqg?via=matrix.org&via=igalia.com&via=tchncs.de\">In reply to</a> <a href=\"https://matrix.to/#/@mhofman:matrix.org\">@mhofman:matrix.org</a><br>Reading all this, I am still curious to understand how Shared Struct help compared to a synchronization mechanism (to implement a thread pool) coupled with an efficient message passing. How much actual shared mutable state is necessary?</blockquote></mx-reply>my thinking has always been single-writer XOR multiple-reader kind of data sharing will get you pretty far","m.relates_to":{"m.in_reply_to":{"event_id":"$cKsSdE1ktI02O2m9ChSMinLZIFcrxm2mNDIP8aBIGqg"}},"msgtype":"m.text"},"ts":1695069042491,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$auOV_-vUHouqyf1G1IS2qZg72nLNNd5THk4eSsDcf5Y"},
{"content":{"body":"I guess I'm wondering how these small data structures for synchronization are used, how much they need to do, and if there's any way to abstract them into higher level concepts. The immutable data could be passed as messages, and does not need to be based on shared struct from what I gather. I am basically still worried we're designing a blunt tool that will be abused when alternatives would be more aligned with the JS ecosystem.","msgtype":"m.text"},"ts":1695069052224,"senderName":"Mathieu Hofman","senderId":"@mhofman:matrix.org","id":"$qYF6-qHdW-fRH6jc-Vq85ZD9I39m3jdBvEvllZ_KmjM"},
{"content":{"body":"but if your application wants mutable shared state there is no alternative","msgtype":"m.text"},"ts":1695069068391,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$4uPX_1ZwBAUsRXojDzim-Z0qbdTqew8lr-ZdLteP8eY"},
{"content":{"body":"i continue to strongly disagree with this handwringing about abuse","msgtype":"m.text"},"ts":1695069136992,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$lXz0T0qcVxFRfJdrjQFg7jD_c7lYoO-K7XE2GZ_jxJY"},
{"content":{"body":"but i think we remain agreed that shared mutable state is a bad thing to entice people into reaching for from the get go","msgtype":"m.text"},"ts":1695069839607,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$qrWNnMV3giHAzraSNvlkiq-uRdYKuJapJLsCFD0wPzo"},
{"content":{"body":"> <@shuyuguo:matrix.org> i was thinking the latter, and scheduling\n\nFor TypeScript, I'm not using postMessage at all except for the built-in one NodeJS does to pass the initial value of `workerData`, so that wouldn't be the cause. ","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$nIhmTRfrA_tVzEqW1Pn2OCslDKvZZW7ZJoMY9GXe0f4?via=matrix.org&via=igalia.com&via=tzchat.org\">In reply to</a> <a href=\"https://matrix.to/#/@shuyuguo:matrix.org\">@shuyuguo:matrix.org</a><br />i was thinking the latter, and scheduling</blockquote></mx-reply>For TypeScript, I'm not using postMessage at all except for the built-in one NodeJS does to pass the initial value of <code>workerData</code>, so that wouldn't be the cause.","m.relates_to":{"m.in_reply_to":{"event_id":"$nIhmTRfrA_tVzEqW1Pn2OCslDKvZZW7ZJoMY9GXe0f4"}},"msgtype":"m.text"},"ts":1695070837075,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$Q8unRAGLeyHWxHBIinfb_RA1w-fQ9tC5Opp6-WhjaSg"},
{"content":{"body":"> <@mhofman:matrix.org> I guess I'm wondering how these small data structures for synchronization are used, how much they need to do, and if there's any way to abstract them into higher level concepts. The immutable data could be passed as messages, and does not need to be based on shared struct from what I gather. I am basically still worried we're designing a blunt tool that will be abused when alternatives would be more aligned with the JS ecosystem.\n\nThe problem is that concurrency and coordination often requires far more complex coordination primitives than we are likely to ship in the standard library. With the implementation in the origin trial, I can easily build these more complex coordination capabilities out of the primitives we have through the use of mutable shared state. If we are limited to only a few built-in mutable and shareable data structures and everything else is immutable, then it is possible this proposal won't meet the needs of the applications that need this capability the most.","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!fmLqwwrfASaCrzNjAk:matrix.org/$qYF6-qHdW-fRH6jc-Vq85ZD9I39m3jdBvEvllZ_KmjM?via=matrix.org&via=igalia.com&via=mozilla.org\">In reply to</a> <a href=\"https://matrix.to/#/@mhofman:matrix.org\">@mhofman:matrix.org</a><br>I guess I&#39;m wondering how these small data structures for synchronization are used, how much they need to do, and if there&#39;s any way to abstract them into higher level concepts. The immutable data could be passed as messages, and does not need to be based on shared struct from what I gather. I am basically still worried we&#39;re designing a blunt tool that will be abused when alternatives would be more aligned with the JS ecosystem.</blockquote></mx-reply>The problem is that concurrency and coordination often requires far more complex coordination primitives than we are likely to ship in the standard library. With the implementation in the origin trial, I can easily build these more complex coordination capabilities out of the primitives we have through the use of mutable shared state. If we are limited to only a few built-in mutable and shareable data structures and everything else is immutable, then it is possible this proposal won't meet the needs of the applications that need this capability the most.","m.relates_to":{"m.in_reply_to":{"event_id":"$qYF6-qHdW-fRH6jc-Vq85ZD9I39m3jdBvEvllZ_KmjM"}},"msgtype":"m.text"},"ts":1695071087007,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$jwL01yQqeMt-bOqDTiEpbNkOoIoreMZMkBS6RoSjAJw"},
{"content":{"body":"That's not saying we shouldn't *also* have immutable data structures, or at least the ability to freeze all or part of a shared struct, as I'd like those too.","format":"org.matrix.custom.html","formatted_body":"That's not saying we shouldn't <em>also</em> have immutable data structures, or at least the ability to freeze all or part of a shared struct, as I'd like those too.","msgtype":"m.text"},"ts":1695071156119,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$YK4E703mGqqTNwEBJLuea5fqrK8ZPJK24pH6kfXN-mo"},
{"content":{"body":"rbuckton: yeah that all tracks exactly with my intuition","format":"org.matrix.custom.html","formatted_body":"<a href=\"https://matrix.to/#/@rbuckton:matrix.org\">rbuckton</a>: yeah that all tracks exactly with my intuition","msgtype":"m.text"},"ts":1695071180693,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$K9PLJeJKFe_MpkFbmzqIJJcEy2f2N5lywuuN-noqEIE"},
{"content":{"body":"Even though I would consider most of the TypeScript AST to be immutable, that's not exactly true. It's immutable to our consumers, but we need to be able to attach additional shared data ourselves.","msgtype":"m.text"},"ts":1695071211107,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$pUrrBetif8QKifFXfJRoPFWmuo6ui4Kt3xsi0h2_9TQ"},
{"content":{"body":"for example, I may build a `SourceFile` and its AST in parallel parse, but this file hasn't been bound and had its symbols and exports recorded yet. Once parse is complete, we hand the entire program off to the binder which could also do its work in parallel.","format":"org.matrix.custom.html","formatted_body":"for example, I may build a <code>SourceFile</code> and its AST in parallel parse, but this file hasn't been bound and had its symbols and exports recorded yet. Once parse is complete, we hand the entire program off to the binder which could also do its work in parallel.","msgtype":"m.text"},"ts":1695071285886,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$cM5grddFEYkWPVclLyIJL4vq-260R2KA1jGe16lHTyI"},
{"content":{"body":"in the back of my mind i'm still thinking about the viability of dynamic \"ownership\" tracking, for lack of a better word. by \"ownership\" i mean single writer XOR multiple readers","msgtype":"m.text"},"ts":1695071336380,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$RYJSSxiEEusfkFrz34qivk1BPIy7U1pv0dE4Uvol-gY"},
{"content":{"body":"And while our emitter uses tree transformations that produce a new AST for changed subtrees, we still reuse unchanged subtrees as much as possible, and need to attach additional information about how those original nodes should be handled during emit as well. ","msgtype":"m.text"},"ts":1695071378717,"senderName":"rbuckton","senderId":"@rbuckton:matrix.org","id":"$szQX1GwOA5udfGLeucQf2hOIVqRlrBDzzXApgonAgEo"}
]