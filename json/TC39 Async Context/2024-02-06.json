[
{"content":{"body":"Are there any updates for this proposal? I don't think we've had a meeting in a few weeks (or I missed it), and I don't see it on the agenda ","m.mentions":{},"msgtype":"m.text"},"ts":1707248816429,"senderName":"ethanarrowood","senderId":"@ethanarrowood:matrix.org","id":"$UBO3A5TM3qltpPuE4R8aiXQ4MjDh-ojJd_nCnvRl0Co"},
{"content":{"body":"there were going to be some API updates, e.g., adding AsyncContext.snapshot.wrap. I guess we didn't make a presentation this meeting, but we should have one next meeting.","m.mentions":{},"msgtype":"m.text"},"ts":1707248868290,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$Juw7SJyJiEnsnoMYEQ3rY7GCklR36agTHsW0N6YYugs"},
{"content":{"body":"There is a lot to do in terms of benchmarking, advancing implementations, design documents, etc.","m.mentions":{},"msgtype":"m.text"},"ts":1707248883702,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$xpMne2mmu9SjHjUC6mn9655v9JUluNEVyrFp3CU1jik"},
{"content":{"body":"Great! Happy to help with some of that if you need; though, I'm inexperienced so happy to continue being a fly-on-the-wall and keep learning.","m.mentions":{},"msgtype":"m.text"},"ts":1707248944408,"senderName":"ethanarrowood","senderId":"@ethanarrowood:matrix.org","id":"$7OC-LcFsnl6zr7UDSsffnOlabVpCfNxraE5rLMQeoU0"},
{"content":{"body":"Chengzhong Wu and Andreu Botella are working on those benchmarking/implementing/design doc parts; maybe they can share the relevant links with you if you want to get involved?","format":"org.matrix.custom.html","formatted_body":"<a href=\"https://matrix.to/#/@legendecas:matrix.org\">Chengzhong Wu</a> and <a href=\"https://matrix.to/#/@abotella:igalia.com\">Andreu Botella</a> are working on those benchmarking/implementing/design doc parts; maybe they can share the relevant links with you if you want to get involved?","m.mentions":{"user_ids":["@legendecas:matrix.org","@abotella:igalia.com"]},"msgtype":"m.text"},"ts":1707249036013,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$yd0fVXUlOlKb4nM5K9Daw19lOJp4l83KSOF6viMvW1E"},
{"content":{"body":"oh, I forgot to mention this, but I talked over with Shu, and it looks like they're concerned about memory usage, so I'm currently investigating a linked list implementation of the AsyncContext snapshot","msgtype":"m.text"},"ts":1707249145193,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$AYLZ3LCTwV93QDDuJg-vciEwPrphuJa9UHb4WbLzniE"},
{"content":{"body":" * oh, I forgot to mention this here, but I talked over with Shu, and it looks like they're concerned about memory usage, so I'm currently investigating a linked list implementation of the AsyncContext snapshot","m.new_content":{"body":"oh, I forgot to mention this here, but I talked over with Shu, and it looks like they're concerned about memory usage, so I'm currently investigating a linked list implementation of the AsyncContext snapshot","msgtype":"m.text"},"m.relates_to":{"event_id":"$AYLZ3LCTwV93QDDuJg-vciEwPrphuJa9UHb4WbLzniE","rel_type":"m.replace"},"msgtype":"m.text"},"ts":1707249158158,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$4zITsaNlLtSWOQptbgIKrhlVPin0lnPamRDuDTbS5ug"},
{"content":{"body":"did you consider the data structure that I was suggesting, which also avoids quadratic memory usage?","m.mentions":{},"msgtype":"m.text"},"ts":1707249176588,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$HGHfNT9fF2KG6KjAdbJuapMte8pZhqPVmRegeQX8cQM"},
{"content":{"body":"I'm going to build implementations of both and compare","msgtype":"m.text"},"ts":1707249193365,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$ayt4yWh4uQFm_qkg1jNh_HgisNwmLZCJEg1PaCKDSA4"},
{"content":{"body":"and continue with the design doc afterwards","msgtype":"m.text"},"ts":1707249207622,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$cBqysG8k5opLCReJuJ5tUyZkQyqyUAPTGjPRkpnjgos"},
{"content":{"body":"I'm pretty confident that linked list is incomplete, and that we need something that gets the benefits of both","m.mentions":{},"msgtype":"m.text"},"ts":1707249212235,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$xcPISGSGjK3bqhDCihqB3ef353S1sUSSFDulYhcxz6A"},
{"content":{"body":"maybe a design doc would be a good place to discuss various data structures?","m.mentions":{},"msgtype":"m.text"},"ts":1707249229594,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$SqlxfX2sgbmv1h-GQGPObmCiOJRuw_a5V2aLr8UqpYk"},
{"content":{"body":"I guess maybe I don't know what you mean by linked list","m.mentions":{},"msgtype":"m.text"},"ts":1707249235984,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$xRs74bLCKtpVa_ey2Oy56v5BE1yjf6deXL5PiTPaIKk"},
{"content":{"body":"(I definitely agree with V8 people that the clone-a-map-all-the-time implementation is not great)","m.mentions":{},"msgtype":"m.text"},"ts":1707249262053,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$0hds9Nwrl2uTlomcgZVLGGaI0FrUP4ahbC0CS3qCfe0"},
{"content":{"body":"we were thinking of a LIFO stack","m.mentions":{},"msgtype":"m.text"},"ts":1707249264164,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$uiV6dJ2LijFp0j_ABT-m0o6wsO9BOkbsMqZgUNNFe9o"},
{"content":{"body":"which you could use a linked list for","m.mentions":{},"msgtype":"m.text"},"ts":1707249267323,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$SI9ZhuIqUnER8F5eaB1rFBAe7zR2StE-UBf0FYlIrCg"},
{"content":{"body":"but the point is that you'd have a cursor into the LIFO stack to propagate instead of a map clone","m.mentions":{},"msgtype":"m.text"},"ts":1707249287488,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$H3X5AbTZBquvOaPfrPg8lgnHvd4Qp7n1eNCAsjPGNlE"},
{"content":{"body":"anyway design doc would be great","m.mentions":{},"msgtype":"m.text"},"ts":1707249293085,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$UcV0ARMsnSeISfwrdYJTQUrMj0eGDMKmOx8gZpYfn-c"},
{"content":{"body":"and that could get flattened when the lookup cost becomes big enough","msgtype":"m.text"},"ts":1707249311191,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$hEdkQmGhpye30PwDJBbjJa55Scp6D1Eu3vKZyTil7Y4"},
{"content":{"body":" * and that could get flattened into a map when the lookup cost becomes big enough","m.new_content":{"body":"and that could get flattened into a map when the lookup cost becomes big enough","msgtype":"m.text"},"m.relates_to":{"event_id":"$hEdkQmGhpye30PwDJBbjJa55Scp6D1Eu3vKZyTil7Y4","rel_type":"m.replace"},"msgtype":"m.text"},"ts":1707249319189,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$T8Wot-ndHzrk_kwUmHYUi7WXRH2QpIFRpwjvSKwCgo8"},
{"content":{"body":"yeah, this seems pretty simplistic and would perform poorly if you need to read or .run on something that's further down in the stack (.run because I assume you'd deduplicate)","m.mentions":{},"msgtype":"m.text"},"ts":1707249320691,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$mVySWb6F0wPnTe0cq0YVx9_xskOOuDq02b_hc1sPj38"},
{"content":{"body":"as far as I can tell, a map lookup in my map implementation is O(N) in the map's capacity, and the stack could be flattened into a map if the lookup for any variable would be greater than O(N)","msgtype":"m.text"},"ts":1707249392152,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$MDBva-uj3pjFz2zaei5yf-apuFzuqk7Iy7S0XKXfv6E"},
{"content":{"body":"and poor performance would limit the applicability of the mechanism (e.g., for incumbent realms, or priorities, or task attribution, or other things in the browser)","m.mentions":{},"msgtype":"m.text"},"ts":1707249402242,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$q7HfhqB2816Q9j8Q8OOzu-m8aTPIv8yqMwsYsspycCY"},
{"content":{"body":" * as far as I can tell, a map lookup in my map implementation is worst-case O(N) in the map's capacity, and the stack could be flattened into a map if the lookup for any variable would be greater than O(N)","m.new_content":{"body":"as far as I can tell, a map lookup in my map implementation is worst-case O(N) in the map's capacity, and the stack could be flattened into a map if the lookup for any variable would be greater than O(N)","msgtype":"m.text"},"m.relates_to":{"event_id":"$MDBva-uj3pjFz2zaei5yf-apuFzuqk7Iy7S0XKXfv6E","rel_type":"m.replace"},"msgtype":"m.text"},"ts":1707249439050,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$iW7Zf7rVTxRkQBtgcxsTk8RUWqKXZ29YD9RHWmIo-BM"},
{"content":{"body":"I think you'd want, at a minimum, to have a segment which is just an array of a fixed maximum size, which is just copied, and then some purely functional mechanism on top (maybe linked list up to a size maximum, but then become a persistent map beyond a certain size?)","m.mentions":{},"msgtype":"m.text"},"ts":1707249466114,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$E0rfjdmvAas96WE-m7PVijl7GR90Knv1-FIVmwdj4Lg"},
{"content":{"body":"i don't understand that yet","m.mentions":{},"msgtype":"m.text"},"ts":1707249475912,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$sqq05K4P-3wdzy9AOezF3al_yCS4f8bKgFn2Ze5BHSU"},
{"content":{"body":"which part?","m.mentions":{},"msgtype":"m.text"},"ts":1707249482954,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$EyIkiu3a9fZuatOdANatcZo_GCSi6IpiUjoLanT75s0"},
{"content":{"body":"but all the more reason for a design doc i guess","m.mentions":{},"msgtype":"m.text"},"ts":1707249484932,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$AyCrccJ5cFihbn8bjfUXejbRpHHqwzvvKhHxhWfiZm4"},
{"content":{"body":"> <@littledan:matrix.org> yeah, this seems pretty simplistic and would perform poorly if you need to read or .run on something that's further down in the stack (.run because I assume you'd deduplicate)\n\nthis thing","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!eQuZUAhGqudVFPodUG:matrix.org/$mVySWb6F0wPnTe0cq0YVx9_xskOOuDq02b_hc1sPj38?via=matrix.org&via=mozilla.org&via=igalia.com\">In reply to</a> <a href=\"https://matrix.to/#/@littledan:matrix.org\">@littledan:matrix.org</a><br>yeah, this seems pretty simplistic and would perform poorly if you need to read or .run on something that&#39;s further down in the stack (.run because I assume you&#39;d deduplicate)</blockquote></mx-reply>this thing","m.mentions":{"user_ids":["@littledan:matrix.org"]},"m.relates_to":{"m.in_reply_to":{"event_id":"$mVySWb6F0wPnTe0cq0YVx9_xskOOuDq02b_hc1sPj38"}},"msgtype":"m.text"},"ts":1707249510267,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$Cq1XOAwzC4B1bBTYqFvgBVYMvDUmSDV7O_iVbv3JpdU"},
{"content":{"body":"you think the on-demand flattening is what would perform badly?","m.mentions":{},"msgtype":"m.text"},"ts":1707249529484,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$fGlL04ZZLhNJymeCRuxcw-GUVcbSz5PjJxNjx-L8glk"},
{"content":{"body":"(because at that point you lose the ability to just propagate a pointer into the stack?)","m.mentions":{},"msgtype":"m.text"},"ts":1707249541811,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$Ml9ZATTaXrHAm6oGe3f0M6bNC9gYrlkdSEFN997-FGo"},
{"content":{"body":"oh I didn't understand the flattening part","m.mentions":{},"msgtype":"m.text"},"ts":1707249554072,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$fOWgrsb25C86xnHzD_6WQjTU-8B7tRtmVz_LBV_Yb98"},
{"content":{"body":"I wasn't even thinking of flattening on demand, I was thinking of flattening when pushing onto the stack","msgtype":"m.text"},"ts":1707249581005,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$TSkDdAfwjraUFlEVbnTNVhrGZKm_-E4K7ZrnF4ShGVw"},
{"content":{"body":" * I wasn't even thinking of flattening on demand on lookup, I was thinking of flattening when pushing onto the stack","m.new_content":{"body":"I wasn't even thinking of flattening on demand on lookup, I was thinking of flattening when pushing onto the stack","msgtype":"m.text"},"m.relates_to":{"event_id":"$TSkDdAfwjraUFlEVbnTNVhrGZKm_-E4K7ZrnF4ShGVw","rel_type":"m.replace"},"msgtype":"m.text"},"ts":1707249589157,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$YPZpyz6eDJxw9XMXJ0TBUTfDNqQMufa3X8dTrk3nIcI"},
{"content":{"body":"yeah let's not confuse each other further and let's get a doc started","m.mentions":{},"msgtype":"m.text"},"ts":1707249605122,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$KyUeQTk3oR_rbi7s54CjgFyYmcPEvn7RXp1ErSbBC2s"},
{"content":{"body":"our assumption was flattening on demand","m.mentions":{},"msgtype":"m.text"},"ts":1707249614056,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$yGgkjM2pBKQboS_AQuLI79ceP6hD0n45qoZBgsvnF-A"},
{"content":{"body":"where that \"demand\" is may be art","m.mentions":{},"msgtype":"m.text"},"ts":1707249624448,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$IS6kA8iZbyfnwYJkfMy6feh0lmNzDRylAnbnrXzuHyw"},
{"content":{"body":"do you imagine de-duplicating only during that flattening operation? then this could have GC implications","m.mentions":{},"msgtype":"m.text"},"ts":1707249649819,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$1ZYB5p6wvwDM_w3WB3bgcIzyR7tUR743laYTm-4yZhw"},
{"content":{"body":"good question, dunno","m.mentions":{},"msgtype":"m.text"},"ts":1707249749093,"senderName":"shu","senderId":"@shuyuguo:matrix.org","id":"$RTgF2RyQLBOLS-K2kot3CoHebH3uaMa7zaf-wp2rEcs"},
{"content":{"body":"the way I was thinking about it, `.get()` needs to be a fast operation, and if you flatten there, with amortization it can't be faster than a map lookup","format":"org.matrix.custom.html","formatted_body":"the way I was thinking about it, <code>.get()</code> needs to be a fast operation, and if you flatten there, with amortization it can't be faster than a map lookup","msgtype":"m.text"},"ts":1707249917683,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$OJc5AKfl8OHU0sDIqMj1MorH9WojSoy8DR-CKP0gmXI"},
{"content":{"body":"whereas `AsyncContext.Variable.p.run()` is not necessarily expected to be fast","format":"org.matrix.custom.html","formatted_body":"whereas <code>AsyncContext.Variable.p.run()</code> is not necessarily expected to be fast","msgtype":"m.text"},"ts":1707249942254,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$sh8V985nk8w9u18eJ73nPTcBLTirfGncHL5Y8YutPBM"},
{"content":{"body":"but yeah, I hadn't considered those GC implications","msgtype":"m.text"},"ts":1707250504473,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$SDX1y5kAEE7ls7DX-22bGDQHg5wzWEkAOe3fpzwpdQE"},
{"content":{"body":"I think `.get()` should be fast, and we can slow down `.run()`","format":"org.matrix.custom.html","formatted_body":"I think <code>.get()</code> should be fast, and we can slow down <code>.run()</code>","m.mentions":{},"msgtype":"m.text"},"ts":1707251073501,"senderName":"Justin Ridgewell","senderId":"@jridgewell:matrix.org","id":"$Cf1bmIc6kt1rK6khmkAu-Yq3128EWM_6fF3iKtGsC4w"},
{"content":{"body":"What does a LIFO stack really give us for memory?","m.mentions":{},"msgtype":"m.text"},"ts":1707251129497,"senderName":"Justin Ridgewell","senderId":"@jridgewell:matrix.org","id":"$zOLuVOJwI1WgrMcAuWK1Yqlw-_xMEic2Xni3iGgY1Kw"},
{"content":{"body":"Is it just the `.run()` operation being faster?","format":"org.matrix.custom.html","formatted_body":"Is it just the <code>.run()</code> operation being faster?","m.mentions":{},"msgtype":"m.text"},"ts":1707251141317,"senderName":"Justin Ridgewell","senderId":"@jridgewell:matrix.org","id":"$Vnx8LMlm2T3g_1_QinrxTIE_g7fatEBN-G-ote-K7OM"},
{"content":{"body":"(There's a suggested impl in the https://github.com/tc39/proposal-async-context/tree/master/src which doesn't perform a clone unless necessary)","m.mentions":{},"msgtype":"m.text"},"ts":1707251177491,"senderName":"Justin Ridgewell","senderId":"@jridgewell:matrix.org","id":"$oCqDNHTvEZZjLIFKNl8qz7nXAODWtlOEpaUMYVYgSRU"},
{"content":{"body":"* (There's a demo impl in the https://github.com/tc39/proposal-async-context/tree/master/src which doesn't perform a clone unless necessary)","m.new_content":{"body":"(There's a demo impl in the https://github.com/tc39/proposal-async-context/tree/master/src which doesn't perform a clone unless necessary)","msgtype":"m.text"},"m.relates_to":{"event_id":"$oCqDNHTvEZZjLIFKNl8qz7nXAODWtlOEpaUMYVYgSRU","rel_type":"m.replace"},"msgtype":"m.text"},"ts":1707251457077,"senderName":"Justin Ridgewell","senderId":"@jridgewell:matrix.org","id":"$FUSuFOgPnNqUtpUc30OvdCDiVUHER5KMJNtYERpWnBA"},
{"content":{"body":"> <@jridgewell:matrix.org> I think `.get()` should be fast, and we can slow down `.run()`\n\nI think both of these should be somewhat fast and memory-efficient, and you're imagining an either-or tradeoff where we can really do well in both ways","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!eQuZUAhGqudVFPodUG:matrix.org/$Cf1bmIc6kt1rK6khmkAu-Yq3128EWM_6fF3iKtGsC4w?via=matrix.org&via=mozilla.org&via=igalia.com\">In reply to</a> <a href=\"https://matrix.to/#/@jridgewell:matrix.org\">@jridgewell:matrix.org</a><br>I think <code>.get()</code> should be fast, and we can slow down <code>.run()</code></blockquote></mx-reply>I think both of these should be somewhat fast and memory-efficient, and you're imagining an either-or tradeoff where we can really do well in both ways","m.mentions":{"user_ids":["@jridgewell:matrix.org"]},"m.relates_to":{"m.in_reply_to":{"event_id":"$Cf1bmIc6kt1rK6khmkAu-Yq3128EWM_6fF3iKtGsC4w"}},"msgtype":"m.text"},"ts":1707253230370,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$fpSQh-cO-MIKo53c5DCde3n5ETIsnMXvjB0PAOYxH1o"},
{"content":{"body":"task attribution involves lots of .run's. I think we'll run into more cases like this over time. I understand that your case doesn't involve .run as frequently, though.","m.mentions":{},"msgtype":"m.text"},"ts":1707253279695,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$7oiOgECvbexqB1umDEhPk9xNILtI4wL2FWy3t-JM5Bk"},
{"content":{"body":"Doesn’t attribution involve at least one get for every run?","msgtype":"m.text"},"ts":1707253438830,"senderName":"Justin Ridgewell","senderId":"@jridgewell:matrix.org","id":"$M5fol3YSt1hTKd_3zE_xC2joz1CLfbWg_RC4u8FvJEk"},
{"content":{"body":"yes, so if .get is fast and .run is really slow, the result is really slow...","m.mentions":{},"msgtype":"m.text"},"ts":1707254112232,"senderName":"littledan","senderId":"@littledan:matrix.org","id":"$xvzYi3F2AQWrMEPkcsYlMnCxikTthrR_4r9DES6Jse4"},
{"content":{"body":"> <@littledan:matrix.org> task attribution involves lots of .run's. I think we'll run into more cases like this over time. I understand that your case doesn't involve .run as frequently, though.\n\nthey're working to not require that many run's","format":"org.matrix.custom.html","formatted_body":"<mx-reply><blockquote><a href=\"https://matrix.to/#/!eQuZUAhGqudVFPodUG:matrix.org/$7oiOgECvbexqB1umDEhPk9xNILtI4wL2FWy3t-JM5Bk?via=matrix.org&via=mozilla.org&via=igalia.com\">In reply to</a> <a href=\"https://matrix.to/#/@littledan:matrix.org\">@littledan:matrix.org</a><br>task attribution involves lots of .run&#39;s. I think we&#39;ll run into more cases like this over time. I understand that your case doesn&#39;t involve .run as frequently, though.</blockquote></mx-reply>they're working to not require that many run's","m.relates_to":{"m.in_reply_to":{"event_id":"$7oiOgECvbexqB1umDEhPk9xNILtI4wL2FWy3t-JM5Bk"}},"msgtype":"m.text"},"ts":1707254786026,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$FOSOY_s3Yh17-tkFecguzL_6MiPTmhWiRHir70XfBu8"},
{"content":{"body":"https://docs.google.com/document/d/1hZ1FdFtHoPk7h9mwTPJSlF83T7YnTpmfa0CEQbPn8Ks/edit#heading=h.h6xaqbodqfo3","msgtype":"m.text"},"ts":1707254844849,"senderName":"Andreu Botella","senderId":"@abotella:igalia.com","id":"$DFF2bjpOUtHQnz8lLOHtwZrsiR0O_fYQsTXGCRXadDM"}
]