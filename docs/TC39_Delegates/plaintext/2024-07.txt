2024-07-01
[09:11:20.0025] <Chris de Almeida>
reviewers need write access if you want the green check mark / meet the requirements for reviewer merge approval if desired

[10:40:21.0294] <Rob Palmer>
The interest survey for TC39 Plenary in Tokyo in October is posted üéâ

- [Interest Survey link](https://github.com/tc39/Reflector/issues/534) üáØüáµ

Please respond by Tuesday 9th July. It only takes ~45s to complete.

[10:40:44.0653] <Rob Palmer>
* The interest survey for TC39 Plenary in Tokyo in October is posted üéâ
Interest Survey link üáØüáµ
Please respond by Tuesday 9th July. It only takes ~45s to complete.


[10:41:15.0849] <Rob Palmer>
* The interest survey for TC39 Plenary in Tokyo in October is posted üéâ

- [Interest Survey link](https://github.com/tc39/Reflector/issues/534) üáØüáµ

Please respond by Tuesday 9th July. It only takes ~45s to complete.


2024-07-05
[20:38:47.0429] <bakkot>
not immediately relevant to JS as a language, but the JS promise integration for wasm is an interesting project worth reading about I think https://v8.dev/blog/jspi 

[00:02:43.0671] <Rob Palmer>
This looks like automatic conditional `await` of exported Wasm functions without user intervention. Meaning the JS caller does not need to be an async function, and calls the Wasm function as if it were sync.

If this is true, does it means that we have solved JS sync functions calling JS async function in a sync manner? Because you just need to insert a thin layer of Wasm between them.

[00:23:03.0718] <nicolo-ribaudo>
You still need to await the Wasm function on the JS side. It's implicit only on Wasm.

See `promise_update` in https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md

[00:23:17.0059] <nicolo-ribaudo>
* You still need to await the Wasm function on the JS side. It's implicit only in Wasm.
See promise_update in https://github.com/WebAssembly/js-promise-integration/blob/main/proposals/js-promise-integration/Overview.md


[08:45:40.0252] <Rob Palmer>
The interest survey for TC39 Plenary in Tokyo in October is posted  üéâ

- [Interest Survey link](https://github.com/tc39/Reflector/issues/534) üáØüáµ

Of 25 replies we have 19 people categorized as likely or certain - which is great progress.  Please respond by Tuesday 9th July. It only takes ~45s to complete.


2024-07-08
[08:18:33.0303] <mgaudet>
I was sad to miss Jaakko J√§rvi's talk at the TG5 meeting before Helsinki (https://github.com/tc39/Reflector/issues/528) -- is there any slides/notes from there that I could follow up with? 

[08:21:32.0137] <Michael Ficarra>
@mgaudet:mozilla.org There were slides, and you'd probably be interested in them. You should send an email to his university email (jaakko.jarvi@utu.fi) for them. The notes are... very light: https://github.com/tc39/tg5/pull/10/files

[08:23:29.0456] <mgaudet>
Thank you for the pointers! 

[10:54:57.0299] <Mikhail Barash>
> <@mgaudet:mozilla.org> I was sad to miss Jaakko J√§rvi's talk at the TG5 meeting before Helsinki (https://github.com/tc39/Reflector/issues/528) -- is there any slides/notes from there that I could follow up with? 

I‚Äôll ask Jaakko to share the slides 

[10:55:10.0315] <mgaudet>
(I sent him an email already too :) ) 

[10:56:48.0345] <Mikhail Barash>
> <@mikbar-uib:matrix.org> I‚Äôll ask Jaakko to share the slides 

You mean the C++ talk? Or the slides on HotDrink?

[10:56:53.0951] <mgaudet>
the C_

[10:56:58.0528] <mgaudet>
 * the C++ talk 

[11:07:09.0996] <Mikhail Barash>
> <@michaelficarra:matrix.org> @mgaudet:mozilla.org There were slides, and you'd probably be interested in them. You should send an email to his university email (jaakko.jarvi@utu.fi) for them. The notes are... very light: https://github.com/tc39/tg5/pull/10/files

notes were not taken during Jaakko's talk on C++, as that part of the TG5 Workshop wasn't formally under Ecma's IPR policy...


2024-07-11
[03:34:51.0060] <Rob Palmer>
Hello delegates,

The TC39 Plenary meeting in Tokyo in October is [confirmed](https://github.com/tc39/Reflector/issues/534#issuecomment-2222579237) üéâ

The invite will be posted shortly.


2024-07-14
[14:23:25.0485] <ljharb>
are there any hotel recs for Tokyo yet? (also a matrix channel for attending delegates?)

[15:36:45.0869] <Rob Palmer>
Not yet. I am working on it. 


2024-07-17
[22:35:38.0874] <Ashley Claymore>
https://workspaceupdates.googleblog.com/2024/07/import-and-export-markdown-in-google-docs.html?m=1
üëÄ we might be able to stop fighting docs when taking notes and use actual bullet points 


2024-07-19
[09:39:36.0296] <Chris de Almeida>
We have passed the agenda deadline, but please add any late-breaking items you may have.

Please indicate on the agenda if there is any possibility that you will ask for stage advancement.

Also, please add any schedule constraints as soon as possible.


2024-07-26
[09:14:04.0758] <Rob Palmer>
Tokyo invite for October is posted:  https://github.com/tc39/Reflector/issues/537

[14:51:39.0165] <Chris de Almeida>
Draft schedule is up:
https://hackmd.io/6dSbhC_RRBuR0-8vZDh7Mg?view


2024-07-27
[06:50:02.0087] <nicolo-ribaudo>
> <@softwarechris:matrix.org> Draft schedule is up:
> https://hackmd.io/6dSbhC_RRBuR0-8vZDh7Mg?view

Nice we get a whole free day


2024-07-29
[08:59:09.0806] <shu>
it's T-1 hours and still no VC link?

[09:14:10.0653] <Michael Ficarra>
@shuyuguo:matrix.org VC link is up now

[09:19:27.0646] <snek>
we have to sign into webex?

[09:19:44.0150] <Chris de Almeida>
 * Draft schedule is up:
https://github.com/tc39/Reflector/issues/536

[09:19:51.0543] <Chris de Almeida>
no

[09:21:06.0084] <snek>
oh i see. the screen saying sign in with existing account got me üòÑ

[09:22:55.0071] <Chris de Almeida>
and you don't have to use the app either -- you can use browser 

[09:48:45.0327] <Rob Palmer>
The meeting will begin in 11 minutes.  There are seven of us in the call now.

[09:48:58.0636] <Aki>
i'm here! hi!

[09:49:09.0591] <Aki>
i'm in the self-imposed lobby bc i'm on the phone

[09:49:43.0302] <Rob Palmer>
Is the lobby music good?  Do you need any any help progressing?

[09:50:17.0082] <Aki>
lol thank you webex, i noticed: https://snaps.akiro.se/2407_ua0hr.jpg

[09:50:59.0140] <snek>
could just have the meeting in the lobby

[09:57:11.0856] <Rob Palmer>
3 minutes... encounting

[09:57:46.0769] <Aki>
_encounting_ ?

[09:58:53.0979] <Rob Palmer>
(I will find you the relevant movie clip soon)

[10:03:41.0690] <snek>
rob you're roboting a lot

[10:04:34.0825] <snek>
 maybe turn your video off?

[10:10:18.0185] <Rob Palmer>
I am getting a new router delivered tomorrow.

[10:17:09.0413] <shu>
my webex cuts out for like 5s every 2 mins or so

[10:17:22.0991] <shu>
i wonder if it's a web version problem or a corp wifi problem...

[10:17:23.0999] <jkup>
Wow 4 new members üöÄ

[10:18:33.0982] <snek>
i think its a web version problem

[10:20:05.0012] <shu>
i don't want to install the native client but all right

[10:40:35.0160] <kriskowal>
But we can say, Welcome Back *Editor* Ben.

[10:41:07.0699] <Michael Ficarra>
Chip had me in the first half

[10:41:31.0078] <shu>
i got a new coin you gotta hear about then

[10:52:33.0228] <nicolo-ribaudo>
fomo

[10:54:52.0541] <nicolo-ribaudo>
I wanted to say something funny but my mic is not working

[10:54:57.0383] <nicolo-ribaudo>
Thanks everybody!

[10:55:15.0149] <nicolo-ribaudo>
Somebody say that I say thanks :P

[11:13:11.0455] <peetk>
should there be a `String.prototype.chars` for when you really do want to iterate over a string's characters

[11:13:52.0376] <bakkot>
"characters" is a wildly overloaded term

[11:14:05.0275] <bakkot>
there should be a `String.prototype.codepoints` to iterate over a string's codepoints

[11:14:24.0889] <bakkot>
I'm less convinced of a `.graphemes` because that's a 402 thing but it could perhaps be done

[11:14:51.0478] <bakkot>
I am not sure there's any use cases for a `.codeunits` but if you have a strong use case that could be done as well

[11:15:09.0875] <snek>
my brain assumed it would be `.values`

[11:15:27.0411] <snek>
but i guess [Symbol.iterator]() is a better way to spell that

[11:16:58.0301] <peetk>
yea mb i meant codepoints

[11:24:27.0152] <nicolo-ribaudo>
My mic keeps not working in webex even with different browsers and after rebooting üò≠

[11:28:04.0906] <nicolo-ribaudo>
Oh it works on my ipad

[11:28:06.0444] <nicolo-ribaudo>
Good

[11:28:44.0858] <keith_miller>
When we decide that we want to change the behavior of spec operations like `toFixed` do we comment or rename them to say that they're legacy?

[11:29:47.0166] <keith_miller>
If we don't, would it be worthwhile to do that so we don't accidentally slip in new uses in the future? Or we could have a spec linter rule, idk if such a tool exists?

[11:30:33.0844] <bakkot>
yeah, probably we should rename the existing ones and add new stricter ones

[11:30:42.0416] <bakkot>
or add a "looseness" parameter, I guess

[11:32:10.0572] <littledan>
Are there other changes that we would make to Temporal if we wanted to apply all the conventions?

[11:32:21.0792] <bakkot>
not doing any coercions at all :)

[11:32:29.0498] <bakkot>
which would save a lot of tests, among other things

[11:32:44.0530] <littledan>
I meant for the conventions that we already adopted

[11:32:56.0039] <bakkot>
we already adopted "don't do any coercions"

[11:33:03.0692] <littledan>
Oh right

[11:33:19.0204] <littledan>
I am in favor of applying this to Temporal

[11:34:08.0148] <littledan>
I think it is much more likely that we can apply this change to 402.

[11:34:22.0023] <littledan>
We aren‚Äôt talking about 262 overall, just those three methods

[11:35:27.0453] <ptomato>
FWIW for Temporal.Duration we already rejected non-integral inputs because of people putting in values like `Temporal.Duration.from({ hours: 1.5 })` 

[11:37:28.0463] <ptomato>
I'm open to going back and changing other things like `roundingIncrement`, `fractionalSecondDigits`, and `Temporal.PlainDate.from({ year: 2024.5, month: 7.5, day: 29.5 })` to reject non-integral inputs but I'd like us to explicitly approve that now, rather than me coming back to a following meeting with a PR

[11:38:23.0817] <littledan>
> <@pchimento:igalia.com> I'm open to going back and changing other things like `roundingIncrement`, `fractionalSecondDigits`, and `Temporal.PlainDate.from({ year: 2024.5, month: 7.5, day: 29.5 })` to reject non-integral inputs but I'd like us to explicitly approve that now, rather than me coming back to a following meeting with a PR

What about the other conventions?

[11:38:54.0222] <ptomato>
I don't know off the top of my head how much deviation there is from the other conventions, I'd have to check

[11:39:14.0988] <rbuckton>
IMO, it makes sense that year, month, and day require integral inputs, but not hour/minute/second. Given one of the goals of Temporal is to make date and time math easier, not being able to write `Temporal.Duration.from({ hours: 1.5 })` is unfortunate. 

[11:41:25.0193] <ptomato>
should `Temporal.Duration.from({ hours: 1.15 })` be 1.15 hours or 1.1499999999999999112 hours? those are distinct nanosecond values

[11:41:57.0556] <bakkot>
> <@littledan:matrix.org> What about the other conventions?

Are there any? Other than the new strings-aren't-iterable one I guess. https://github.com/tc39/how-we-work/blob/main/normative-conventions.md

[11:42:46.0006] <littledan>
Well I was wondering if ptomato had an opinion on the ‚Äúnot coercing at all‚Äù one with respect to Temporal

[11:43:05.0143] <rbuckton>
I regularly want to be able to easily scale a duration to implement backoff, and there's already no convenient way to scale a fixed duration (e.g., hours and smaller). 

[11:44:11.0491] <ptomato>
> <@littledan:matrix.org> Well I was wondering if ptomato had an opinion on the ‚Äúnot coercing at all‚Äù one with respect to Temporal

I don't have much of an opinion, but what I don't want is to have to come back to the next meeting with a PR

[11:45:32.0438] <littledan>
> <@pchimento:igalia.com> I don't have much of an opinion, but what I don't want is to have to come back to the next meeting with a PR

I agree, we should be able to agree on the conclusion in principle and then you should just be able to land it with some async review

[11:45:36.0539] <rbuckton>
for backoff I'm more likely to convert a `Duration` input into ms or ns via `total` and just work with the single unit that I can scale appropriately.

[11:46:16.0165] <littledan>
Rbuckton: you are making a very different feature request from this coercion discussion. Probably this is best for the temporal v2 bucket

[11:47:05.0737] <ptomato>
rbuckton: we considered a `Temporal.Duration.prototype.multiply` at one point but decided it was out of scope. if you have an idea for how it should work, could you open an issue on https://github.com/js-temporal/proposal-temporal-v2/? I think the use case would be helpful

[11:47:35.0296] <rbuckton>
> <@littledan:matrix.org> Rbuckton: you are making a very different feature request from this coercion discussion. Probably this is best for the temporal v2 bucket

Perhaps. It started as commentary related to the statement that Temporal rejects non-integral values.

[11:48:11.0154] <ptomato>
oh actually we already have one: https://github.com/js-temporal/proposal-temporal-v2/issues/7

[11:51:38.0985] <bakkot>
littledan ptomato this meeting has some extra time if you want to put together a last-minute item for dropping coercion from temporal, though it is very far past the deadline so it would need to be conditional approval

[11:55:15.0450] <littledan>
Why not stage 4 today for import attributes?

[11:55:47.0656] <littledan>
> <@bakkot:matrix.org> littledan ptomato this meeting has some extra time if you want to put together a last-minute item for dropping coercion from temporal, though it is very far past the deadline so it would need to be conditional approval

Conditional on what?

[11:57:06.0404] <bakkot>
no one subsequently raising an objection at the next meeting once they'd had time to review, I guess

[11:58:09.0853] <shu>
waldemar: what's the issue with the spec text?

[11:58:27.0440] <littledan>
OK, this is a different way of using conditional advancement than we usually do, which is more about conditions which can be met async 

[13:01:07.0443] <Ben>
I'm having trouble with my microphone,  but I'll get in on notes again

[13:03:36.0248] <saminahusain>
Hat's are still available

[13:03:42.0072] <Justin Ridgewell>
I can help in 5ish min

[13:04:52.0095] <Andreu Botella>
I can help until AsyncContext

[13:07:35.0863] <Chengzhong Wu>
How can I register for one? ü§†

[13:08:35.0016] <Michael Ficarra>
I don't recall a linearity consensus

[13:08:44.0228] <Michael Ficarra>
monotonicity only, which is what we have

[13:10:48.0144] <littledan>
I thought exponential backoff was waiting longer and longer (I don't have background in this area)

[13:11:09.0152] <nicolo-ribaudo>
> <@littledan:matrix.org> I thought exponential backoff was waiting longer and longer (I don't have background in this area)

I also thought the same, but wikipedia clearly shows it's the other way around

[13:11:20.0316] <snek>
depends on what you're talking about

[13:11:24.0784] <nicolo-ribaudo>
For some reason I always assumed that pause was monotonically increasing, and not monotonically decreasing

[13:11:28.0951] <snek>
in this case it gets shorter and shorter until falling back to the slow path

[13:11:45.0164] <Chris de Almeida>
> SYG: Okay, understood, I would document that, that the input number is intended to increase linearly.

[13:11:52.0430] <nicolo-ribaudo>
> <@littledan:matrix.org> I thought exponential backoff was waiting longer and longer (I don't have background in this area)

 * I also thought the same, ~~but wikipedia clearly shows it's the other way around~~

[13:11:58.0701] <nicolo-ribaudo>
 * I also thought the same, <del>but wikipedia clearly shows it's the other way around</del>

[13:12:32.0574] <Michael Ficarra>
wait, that's still not talking about the relationship

[13:12:55.0809] <Michael Ficarra>
yes the input can increase linearly, but that doesn't mean it is linearly proportional to the time waited

[13:15:04.0270] <littledan>
saying "the spec is not intelligible" is not an effective way to communicate because it is very unspecific.

[13:15:22.0303] <littledan>
It would be better to separate the discussions of what *should* happen, from how we encode this in the words

[13:15:37.0673] <Michael Ficarra>
also there is no editorial issue with the spec as far as I can tell

[13:15:53.0536] <Michael Ficarra>
we were very careful with the phrasing in this proposal

[13:17:12.0437] <Rob Palmer>
This is reminding me of the coffee `filter` in/out discussion in Nov 2019.  Folk are seeing the pause N from both sides.  Shu wants small N to mean a longer pause.

[13:17:37.0256] <peetk>
i agree with justin that note 3 does not say what shu was saying it should say

[13:18:07.0548] <littledan>
> <@robpalme:matrix.org> This is reminding me of the coffee `filter` in/out discussion in Nov 2019.  Folk are seeing the pause N from both sides.  Shu wants small N to mean a longer pause.

the two discussions are being mixed together, the editorial and normative

[13:18:10.0692] <Michael Ficarra>
note 1 literally says that this means a pause instruction on such architectures lol

[13:18:14.0923] <littledan>
so it is confusing; we need to separate and order them

[13:20:11.0517] <nicolo-ribaudo>
My reading of step 2 is the opposite of what shu is saying, and it's not just the note being the opposite. Assuming that "a signal is sent" means "wait a little bit", for larger Ns it waits for a little bit more times

[13:20:42.0523] <littledan>
I am also confused by the wording, but let's first focus on, what *should* the thing do, and then we can fix/disambiguate the wording

[13:21:15.0690] <Rob Palmer>
I think we have agreement on the normative:  the pauses between spins get bigger and bigger before going to sleep.

[13:21:29.0420] <littledan>
no it was the opposite :)

[13:21:30.0669] <nicolo-ribaudo>
> <@robpalme:matrix.org> I think we have agreement on the normative:  the pauses between spins get bigger and bigger before going to sleep.

Didn't Shu say the opposite?

[13:21:43.0789] <littledan>
also we don't have agreement; Justin is disagreeing on substance

[13:22:44.0011] <littledan>
what is it that Shu is proposing?

[13:22:50.0606] <kriskowal>
We must now decide whether to spin in session or yield to the agenda :P

[13:23:02.0704] <kriskowal>
Context switches are expensive

[13:23:19.0474] <Justin Ridgewell>
Having the smaller-i-longer-wait sematnics is fine with me if it‚Äôs really how other implemenations have done it.

[13:23:35.0543] <snek>
pauses get bigger and smaller in mature implementations. ideally you want to have the behavior that kris explained, but implementations like linux for example also have a signal of "starvation" which can cause the delay to get longer as well.

[13:23:42.0510] <Justin Ridgewell>
The current semantics with reworded text and note would be fine with me.

[13:24:11.0512] <Justin Ridgewell>
The current semantics reads as the opposite of the current spec/note to me.

[13:24:34.0860] <saminahusain>
will you be in Tokyo? I will bring a bunch.

[13:24:57.0379] <littledan>
> <@devsnek:matrix.org> pauses get bigger and smaller in mature implementations. ideally you want to have the behavior that kris explained, but implementations like linux for example also have a signal of "starvation" which can cause the delay to get longer as well.

yeah let's just not give this semantics

[13:25:27.0492] <snek>
yeah i think it would be best if we simply don't constrain it

[13:25:33.0711] <littledan>
this will need an overflow item, there's too much overflow to get through it

[13:25:36.0631] <littledan>
we have a lot to say

[13:26:27.0475] <bakkot>
if we don't give it semantics, then someone will ship (wlog) "longer iteration number is short wait", and then some application be written in such a way that depends on that behavior for performance, and now it is web-reality and can't be changed without negative performance impact

[13:26:36.0404] <bakkot>
so I don't see much benefit from not giving it semantics

[13:26:39.0465] <rbuckton>
spin locks use the counter to make a determination as to whether you've been spinning often enough to justify a context switch/kernel transition. The counter is used to indicate frequency and sometimes introduce and reduce contention. not all counter values are guaranteed to pause in some implementations

[13:26:49.0033] <bakkot>
I guess it allows implementations to specialize for specific scripts, which they do sometimes do, but... ugh

[13:26:58.0995] <bakkot>
 * I guess it allows implementations to give special behavior for specific scripts, which they do sometimes do, but... ugh

[13:27:20.0607] <littledan>
well, this is a lot like tuning GC

[13:28:07.0330] <rbuckton>
Most spinlock/spinwait implementations are very handwavy on specifics because it's CPU architecture dependent

[13:28:46.0517] <Chengzhong Wu>
Likely will be there, thanks!

[13:32:06.0345] <rbuckton>
iteration count is used more to determine `pause` request frequency, not how much time to wait.

[13:34:27.0066] <rbuckton>
If the iteration count is high and you're approaching a context switch, you want to wait less and less time to give the high-iteration spin a chance to attain the lock. But many spin wait operations will decide that it may only pause for any length of time when `iterationCount % 10 === 0` or `iterationCount % 100 === 0`, etc., and return immediately in other cases.

[13:35:52.0627] <rbuckton>
The purpose of the argument is indicate spin frequency and to not always pause.

[13:37:05.0518] <shu>
i am just confused what there is to disagree on, there is literally no observable behavior

[13:37:39.0285] <nicolo-ribaudo>
> <@shuyuguo:matrix.org> i am just confused what there is to disagree on, there is literally no observable behavior

How is the number of signals related to the pause length in nanoseconds?

[13:37:53.0105] <shu>
it's implementation-defined

[13:38:27.0812] <rbuckton>
My understanding of the concerns are that the spec text that is either over detailed, or uses the confusing terminology.

[13:38:51.0431] <shu>
i will remove the text about the backoff and any particular relation between the pause length and the iterationNumber argument

[13:39:28.0445] <shu>
there's another design constraint in play which waldemar and michael saboff seem to be saying: if there is no observable behavior, there should be no argument

[13:39:54.0874] <Justin Ridgewell>
> <@shuyuguo:matrix.org> i will remove the text about the backoff and any particular relation between the pause length and the iterationNumber argument

I think that‚Äôll upset @msaboff ?

[13:40:05.0408] <shu>
Justin Ridgewell: how so?

[13:40:17.0563] <shu>
he said JSC will likely ignore it, which is certainly fine

[13:40:35.0669] <shu>
my personal intention is that until the code is in the optimizing JIT and inlined, it _is_ ignored

[13:40:47.0892] <Justin Ridgewell>
His final statement was that if there‚Äôs no note, the param will just be ignored.

[13:40:49.0505] <littledan>
> <@shuyuguo:matrix.org> he said JSC will likely ignore it, which is certainly fine

presumably it'd be better for the web if you can align on this...

[13:41:02.0133] <shu>
i don't think i do...?

[13:41:13.0703] <shu>
presumably JSC will do the right thing for M-chips

[13:41:25.0322] <shu>
and V8 might do a more generic thing?

[13:41:40.0341] <littledan>
OK sure

[13:42:04.0381] <littledan>
but "just ignore it" doesn't sound like something tuned for M-chips; it sounds like not engaging with the question

[13:42:27.0170] <shu>
> <@littledan:matrix.org> but "just ignore it" doesn't sound like something tuned for M-chips; it sounds like not engaging with the question

fair, but it's intended to be a hint

[13:42:34.0004] <shu>
why would we try to align on how to interpret a hint?

[13:42:39.0511] <shu>
it's supposed to give freedom to implementations!

[13:42:47.0592] <nicolo-ribaudo>
(I don't know if this is actually implementable given the perf constraints)

What if instead of the iteration number we passed an object, and the engine internally counts how many times it sees that object? So that there is no possible expectation connected to "big number" vs "small number"

[13:42:58.0682] <shu>
no thanks...

[13:43:00.0520] <kriskowal>
I approve of every possible steam (locomotive, motor) metaphor as we can cram into this language.

[13:43:04.0386] <littledan>
> <@shuyuguo:matrix.org> why would we try to align on how to interpret a hint?

well, we'd want to align on, engines feeling like they can interpret this as a hint, rather than that it's just something to ignore

[13:43:30.0853] <shu>
> <@littledan:matrix.org> well, we'd want to align on, engines feeling like they can interpret this as a hint, rather than that it's just something to ignore

yes, i agree. but ignoring is a valid interpretation of a hint, do we agree on that?

[13:44:00.0490] <Justin Ridgewell>
I assume the CPU architecture has some semantic meaning for the hint?

[13:44:03.0943] <littledan>
yes, it is valid, but I hope that JS engines can get past the philosophical disagreement that seems to exist right now and agree that this is a potentially usable hint if it makes sense for the architecture

[13:44:17.0629] <Justin Ridgewell>
We can tell engines to ignore it, but how do they map our hint‚Äôs intention to the arch‚Äôs intention?

[13:44:22.0807] <littledan>
since msaboff's statement was not about the architecture

[13:44:40.0700] <shu>
> <@jridgewell:matrix.org> I assume the CPU architecture has some semantic meaning for the hint?

not the CPU arch per se, more the VM

[13:45:05.0377] <rbuckton>
`Atomics.pause` is purely a CPU architecture dependent operation. A conforming implementation is welcome to ignore `iterationCount` and return immediately if it so chooses, though that is the least efficient approach as it most likely results in thread starvation and does not improve contention.
An efficient implementation of `pause` will use the `iterationCount` to avoid thread starvation and optimize contention by *periodically* introducing an asm `pause` or similar instruction based on whatever information is relevant for that architecture. The main goal is to avoid spinning longer than you would have had to wait for a context switch/kernel transition. These optimizations can include, but are not limited to:
- Only `pausing` every *n* iterations
- issuing a `pause` more or less frequently the longer we've been spinning
- issuing a `pause` more or less frequently based on how frequently we've been spinning
- issuing a `pause` more or less frequently as you approach an amount of time since you started spinning that would be equivalent to the time spent waiting for a context switch.

[13:46:06.0892] <Justin Ridgewell>
> <@rbuckton:matrix.org> `Atomics.pause` is purely a CPU architecture dependent operation. A conforming implementation is welcome to ignore `iterationCount` and return immediately if it so chooses, though that is the least efficient approach as it most likely results in thread starvation and does not improve contention.
> An efficient implementation of `pause` will use the `iterationCount` to avoid thread starvation and optimize contention by *periodically* introducing an asm `pause` or similar instruction based on whatever information is relevant for that architecture. The main goal is to avoid spinning longer than you would have had to wait for a context switch/kernel transition. These optimizations can include, but are not limited to:
> - Only `pausing` every *n* iterations
> - issuing a `pause` more or less frequently the longer we've been spinning
> - issuing a `pause` more or less frequently based on how frequently we've been spinning
> - issuing a `pause` more or less frequently as you approach an amount of time since you started spinning that would be equivalent to the time spent waiting for a context switch.

Is this the interpretation you‚Äôre proposing, or is this a written behavior somewhere?

[13:46:07.0180] <rbuckton>
Most implementations of a `pause`-like method I've seen are heavily optimized based on OS, CPU Arch, runtime, clock speed, and other factors and as such can't be easily expressed in an algorithmic form.

[13:46:41.0382] <rbuckton>
This is my observation from looking at spin-wait mechanisms in several languages and runtimes.

[13:46:52.0118] <rbuckton>
This isn't precise copy that I would use in the spec.

[13:47:33.0218] <rbuckton>
This is more to argue that we should put a lot less into the algorithm steps and NOTEs than we currently are.

[13:49:12.0568] <shu>
i think there are only two realistic options:

1. drop the hint argument entirely
2. accept an integer as an hint and say it is implementation-defined how it is used to determine how long to pause

i still think 2 is uncontroversial but clearly it _is_ controversial but not in a way i understand how to make progress on

[13:49:51.0248] <rbuckton>
We should not drop the hint argument. 

[13:50:13.0050] <shu>
i think this would be a strictly worse API for its purpose if it dropped the hint argument, yes

[13:51:27.0471] <Justin Ridgewell>
Would Waldemar be satisifed with bigger-number-longer-wait? Then let the engine handle that to match the VM‚Äôs CPU instructions, eg smaller-number-longer-wait?

[13:51:42.0877] <rbuckton>
The simplest text would be something along the lines of: "An implementation can use the value of `iterationCount` as a hint to determine whether it is advisable to request the CPU pause for an extremely short period of time (TBD) to decrease contention and avoid an expensive context switch." I can look for something better though.

[13:52:39.0393] <rbuckton>
Basically, (2) but with a few examples of what `iterationCount` could be used for and the time scale we are trying to avoid.

[13:54:04.0223] <shu>
> <@jridgewell:matrix.org> Would Waldemar be satisifed with bigger-number-longer-wait? Then let the engine handle that to match the VM‚Äôs CPU instructions, eg smaller-number-longer-wait?

would that question arise? if something is implementation-defined, you can object to implementation-defined, but it'd be odd to object to specific implementations

[13:55:20.0828] <bakkot>
I am confused about why people don't like "semaphore" as the name for this

[13:55:21.0598] <bakkot>
this is the obvious name

[13:55:36.0768] <rbuckton>
I don't think "bigger-number-longer-wait" is correct, TBH.

[13:55:40.0005] <bakkot>
everyone uses this name when independently implementing this exact API, which has happened many times

[13:56:03.0507] <Justin Ridgewell>
I think the implementation is still left to the engine, but the hint now has a known meaning. Whether it uses exponential/quadratic/etc, whether the CPU expect big or large numbers, is left for the implementation.

[13:56:10.0738] <bakkot>
https://www.npmjs.com/package/semaphore
https://www.npmjs.com/package/@shopify/semaphore
https://www.npmjs.com/package/async-sema

[13:56:13.0404] <littledan>
I think just because "semaphore" is often used to mean "binary semaphore"

[13:56:25.0825] <littledan>
but yes I agree that the name makes sense, just trying to explain the confusion

[13:57:29.0152] <Justin Ridgewell>
> <@rbuckton:matrix.org> I don't think "bigger-number-longer-wait" is correct, TBH.

It‚Äôs the most intuitive, and it‚Äôs how I‚Äôve written async-retires before.

[13:57:47.0986] <rbuckton>
It may be helpful to put together links to spin wait implementations in other languages and to papers on spin waiting, contention, and cpu/task scheduling that are relevant.

[13:57:57.0742] <Justin Ridgewell>
I don‚Äôt know whether it matches CPU behavior.

[13:57:59.0940] <rbuckton>
> <@jridgewell:matrix.org> It‚Äôs the most intuitive, and it‚Äôs how I‚Äôve written async-retires before.

This is not async retries, that is the wrong paradigm.

[13:58:41.0466] <Justin Ridgewell>
Sync retries sounds very similar to async retries‚Ä¶

[14:00:24.0230] <shu>
> <@jridgewell:matrix.org> I think the implementation is still left to the engine, but the hint now has a known meaning. Whether it uses exponential/quadratic/etc, whether the CPU expect big or large numbers, is left for the implementation.

it doesn't have a known meaning, that's why it's a hint

[14:01:13.0799] <shu>
the meaning is basically, write this: `for (i; i < spinCount; i++) { TryLock(); Atomics.pause(i); }`, and know the VM will do what it thinks best

[14:01:43.0569] <keith_miller>
I think if we're going to spec something bigger number longer wait is more intuitive. If you want the opposite you can just implement your spin lock in a countdown loop rather than a count up loop

[14:01:56.0954] <shu>
do not think at a lower level, because the JS programmer has no control at that lower level given the multi-tiered execution, etc

[14:02:18.0653] <shu>
keith_miller: my proposal is to say nothing in the spec text. an implementation can simply choose to interpret it as "larger number is longer"

[14:02:39.0050] <snek>
why do people want to specify a certain relationship between loop count and pauses

[14:02:43.0672] <keith_miller>
I think that's the worst outcome

[14:03:05.0526] <rbuckton>
> <@jridgewell:matrix.org> Sync retries sounds very similar to async retries‚Ä¶

The purpose is more like this: "Ideally we shouldn't spin for longer than it would take for a context switch resulting from putting the thread to sleep or waking it up. To help avoid contention, we should periodically `pause` the thread to give another core an opportunity to perform the operation under contention. However, the closer we get to the amount of time it should have taken to put the thread to sleep, we should `pause` less frequently to aggressively avoid that threshold."
In some implementations, like .NET's `SpinWait`, once you reach a certain number of spins you trigger a `sleep(0)` or `sleep(1)`, which is a long wait, and then essentially restart as if the spin count was close to zero again.

[14:03:11.0277] <shu>
> <@keith_miller:matrix.org> I think that's the worst outcome

can you say more?

[14:03:45.0196] <littledan>
we don't specify pauses for GC; we just trust that engines will figure out a good policy

[14:03:47.0309] <keith_miller>
Then we could end up in a world where one implementation does longer waits as count goes up and others do shorter 

[14:04:17.0787] <snek>
that's good, the implementation should do whatever is best on the given machine.

[14:04:19.0840] <rbuckton>
So the "backoff" tends to be something like: short -> shorter -> shortest -> long -> short -> shorter -> shortest -> etc.

[14:04:23.0247] <snek>
 * that's good, the implementation should do whatever is best on the machine its running on.

[14:04:42.0967] <rbuckton>
With a number of "don't even wait at all" operations thrown into the mix.

[14:04:59.0905] <rbuckton>
> <@keith_miller:matrix.org> Then we could end up in a world where one implementation does longer waits as count goes up and others do shorter

That'

[14:05:13.0262] <keith_miller>
I guess my concern is that one workload wants longer and longer waits. And others want shorter and shorter waits

[14:05:23.0431] <rbuckton>
> <@keith_miller:matrix.org> Then we could end up in a world where one implementation does longer waits as count goes up and others do shorter

 * That's perfectly acceptable. An implementation should choose the pause strategy that works best within its own requirements.

[14:05:30.0551] <keith_miller>
And there's no way to know as an implementation a priori which is better

[14:05:43.0402] <shu>
keith_miller: that's fair, and the concession is that you can't actually express that in JS code 

[14:05:53.0783] <shu>
because the call overhead is so high in the interpreter

[14:06:04.0644] <ryzokuken>
ljharb could you add yourself to the queue again?

[14:06:08.0836] <bakkot>
shu: I guess I agree that the actual API doesn't look like exactly like traditional semaphore but the way you use it is exactly the way you use a semaphore 99% of the time

[14:06:11.0088] <rbuckton>
> <@keith_miller:matrix.org> I guess my concern is that one workload wants longer and longer waits. And others want shorter and shorter waits

That's not what you do with spin locking.

[14:06:12.0541] <snek>
i don't think a user of the pause api would want that. that is a concern you have to express in cpu instructions, so js would be a poor choice for that code.

[14:06:45.0965] <keith_miller>
Then we should just not have anything passed by the JS users and the JS engine can infer the loop count if they *really* want to

[14:06:50.0193] <rbuckton>
If you want longer waits, you use a mutex/condvar or futex. Spin waiting generally should be as short as possible to avoid starvation and break contention.

[14:07:07.0432] <rbuckton>
If you want longer waits, you `sleep`, not `pause`.

[14:07:50.0097] <rbuckton>
> <@keith_miller:matrix.org> Then we should just not have anything passed by the JS users and the JS engine can infer the loop count if they *really* want to

How is the JS engine supposed to know when you start spinning and are under contention when that is purely driven by user code?

[14:08:16.0365] <shu>
bakkot: the API as proposed currently is about governing rate limits, not a mutual exclusion building block. i understand conceptually the core mechanism is the same.

[14:08:34.0176] <rbuckton>
"Spin lock" is also the wrong terminology. This is a "spin wait" and is generally used to write lock-free code via compare-and-swap

[14:08:44.0748] <shu>
if i were to propose a mutual exclusion Semaphore, i would not use this protocol

[14:08:48.0141] <rbuckton>
You can write a spin lock using `pause`, but that's not the goal.

[14:09:01.0444] <shu>
it may be that because of this API mismatch, neither Semaphore-like APIs should get the simple name "Semaphore"

[14:09:04.0056] <Luca Casonato>
https://docs.rs/tokio/latest/tokio/sync/struct.Semaphore.html <- thing called semaphore in a non-js language, with a broadly similar api to our proposal. we will investigate other names though! :D

[14:09:05.0967] <littledan>
+1 on Stage 1

[14:09:43.0610] <shu>
it can also be perhaps more simply resolved by just namespacing

[14:09:55.0139] <shu>
`Governor.Semaphore` vs `Locks.Sempahore` or `Atomics.Semaphore` or whatever

[14:10:27.0834] <rbuckton>
In esfx, I use `Semaphore` for the thread coordination primitive, and `AsyncSemaphore` for the async coordination (non multi-threaded) primitive, though `AsyncSemaphore` is still a simple coordination primitive and has now knowledge of async iteration.

[14:10:32.0054] <shu>
> <@lucacasonato:matrix.org> https://docs.rs/tokio/latest/tokio/sync/struct.Semaphore.html <- thing called semaphore in a non-js language, with a broadly similar api to our proposal. we will investigate other names though! :D

that thing's namespaced under `tokio::sync`, right?

[14:11:04.0774] <Luca Casonato>
Yes!

[14:11:18.0815] <bakkot>
> <@rbuckton:matrix.org> In esfx, I use `Semaphore` for the thread coordination primitive, and `AsyncSemaphore` for the async coordination (non multi-threaded) primitive, though `AsyncSemaphore` is still a simple coordination primitive and has now knowledge of async iteration.

(`Semaphore` as proposed wouldn't have knowledge of async iteration; async iteration would take a governor-protocol-implementing-thing, which `Semaphore` would be)

[14:11:19.0108] <shu>
right, my objection is about this being a globally named `Semaphore`

[14:12:06.0455] <bakkot>
`Governor.Semaphore` wfm if we have such a class though fwiw I don't think `Governor` needs to actually exist 

[14:12:55.0618] <nicolo-ribaudo>
Governor is just a protocol right? Like Thenable

[14:13:03.0770] <bakkot>
right

[14:13:12.0181] <bakkot>
but the proposal includes a class, also

[14:13:12.0730] <bakkot>
for some reason

[14:13:21.0112] <Luca Casonato>
bakkot: the helpers! :D

[14:13:31.0791] <keith_miller>
> <@shuyuguo:matrix.org> keith_miller: that's fair, and the concession is that you can't actually express that in JS code

Then the API should provide a mechanism for saying if you want the pauses to get longer and longer or shorter and shorter or we should just have nothing, IMO.

[14:14:13.0485] <littledan>
yeah I agree with Igalia that this stuff is a bit complicated, but investigating APIs during Stage 1 SGTM

[14:14:14.0169] <shu>
keith_miller: but it might not be a linear/superlinear relationship at all, like what Ron was saying with "wait every 10th iteration" or something

[14:14:48.0775] <shu>
keith_miller: can you articulate why an implementation-defined hint is the worst outcome?

[14:14:54.0668] <rbuckton>
https://en.cppreference.com/w/cpp/thread/counting_semaphore
https://learn.microsoft.com/en-us/dotnet/api/system.threading.semaphore?view=net-8.0
https://learn.microsoft.com/en-us/dotnet/api/system.threading.semaphoreslim?view=net-8.0
https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore
https://docs.python.org/3/library/asyncio-sync.html#boundedsemaphore
https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Semaphore.html
https://esfx.js.org/esfx/api/threading-semaphore.html?tabs=ts (NOTE: my implementation, though not heavily used)
https://esfx.js.org/esfx/api/async-semaphore.html?tabs=ts (NOTE: my implementation, though not heavily used)

[14:15:52.0659] <rbuckton>
> <@shuyuguo:matrix.org> keith_miller: but it might not be a linear/superlinear relationship at all, like what Ron was saying with "wait every 10th iteration" or something

Not linear. Here's .NET's version of `pause()` (MIT Licensed): https://github.com/dotnet/runtime/blob/4c58b5a5132cb089b23d32cafe3fcfa7e615a0da/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs#L144

[14:16:38.0397] <rbuckton>
Also: https://github.com/dotnet/runtime/blob/040fde48a75f0c211353f073e4f69e2e31607752/src/coreclr/System.Private.CoreLib/src/System/Threading/Thread.CoreCLR.cs#L139

[14:16:59.0636] <ljharb>
to me a semaphore are those big flags the dude waves on the top of an aircraft carrier

[14:17:41.0559] <ljharb>
like, a way to send messages/signals

[14:18:39.0625] <keith_miller>
From skimming that code it does `Thread.Yield();` so it doesn't provide a hint?

[14:18:41.0263] <rbuckton>
> <@ljharb:matrix.org> to me a semaphore are those big flags the dude waves on the top of an aircraft carrier

That's pretty much the idea. A counting semaphore tracks how many concurrent users are allowed in and blocks new users from entering until someone leaves. You want in? I check the count and if you're allowed in I waive the flag to let you in, otherwise I waive the flag to tell you to stay put. When someone leaves, I waive the flag to let the next person in.

[14:19:04.0215] <bakkot>
> <@rbuckton:matrix.org> https://en.cppreference.com/w/cpp/thread/counting_semaphore
> https://learn.microsoft.com/en-us/dotnet/api/system.threading.semaphore?view=net-8.0
> https://learn.microsoft.com/en-us/dotnet/api/system.threading.semaphoreslim?view=net-8.0
> https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore
> https://docs.python.org/3/library/asyncio-sync.html#boundedsemaphore
> https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Semaphore.html
> https://esfx.js.org/esfx/api/threading-semaphore.html?tabs=ts (NOTE: my implementation, though not heavily used)
> https://esfx.js.org/esfx/api/async-semaphore.html?tabs=ts (NOTE: my implementation, though not heavily used)

Given this list, `Semaphore` really seems like the right name, though I'm fine with `CountingSemaphore` also. shu wdyt about this list?

[14:19:34.0548] <shu>
> <@bakkot:matrix.org> Given this list, `Semaphore` really seems like the right name, though I'm fine with `CountingSemaphore` also. shu wdyt about this list?

i'll rephase my objection to that i will die on the hill of there being a global builtin named `Semaphore` that implements this symbol-based governor protocol

[14:19:54.0246] <shu>
i do not object to using the name `Semaphore`, so long as it is clear that it is not the mutual exclusion building block semaphore

[14:20:09.0648] <bakkot>
the protocol isn't symbol-based

[14:20:11.0750] <keith_miller>
I agree it does sleep periodically but `sleep` is an independent concept.

[14:20:23.0219] <shu>
i thought it has a protocol?

[14:20:32.0647] <bakkot>
it's string-based, at least currently

[14:20:37.0772] <keith_miller>
Which, I assume isn't intended to be part of your proposal?

[14:20:43.0558] <bakkot>
it is, there is a string-named async `acquire` method which gives you an object with a string-named sync `release` method

[14:21:12.0263] <shu>
ah, string, sorry

[14:21:17.0901] <Luca Casonato>
```
interface Governor {
  acquire(): Promise<GovernorToken>;
}

interface GovernorToken {
  release(): void;
  [Symbol.dispose](): void;
}
```

[14:21:30.0111] <Michael Ficarra>
yeah string vs symbol was one of the post-Stage 1 design considerations

[14:21:31.0788] <Luca Casonato>
 * ```
interface Governor {
  acquire(): Promise<GovernorToken>;
}

interface GovernorToken {
  release(): void;
  // or
  [Symbol.dispose](): void;
}
```

[14:21:35.0961] <Luca Casonato>
 * ```ts
interface Governor {
  acquire(): Promise<GovernorToken>;
}

interface GovernorToken {
  release(): void;
  // or
  [Symbol.dispose](): void;
}
```

[14:21:39.0644] <rbuckton>
The `Semaphore` in the slides implements the `Governor` interface, which is very iterator specific

[14:21:42.0086] <bakkot>
(`Symbol.dispose` isn't part of the protocol but should generally be there yes)

[14:21:54.0461] <Michael Ficarra>
> <@lucacasonato:matrix.org> ```ts
> interface Governor {
>   acquire(): Promise<GovernorToken>;
> }
> 
> interface GovernorToken {
>   release(): void;
>   // or
>   [Symbol.dispose](): void;
> }
> ```

and, not or

[14:22:09.0918] <shu>
that `acquire(): Promise<GovernorToken>` is not how i would declare `acquire` if i were designing a shmem semaphore

[14:22:26.0367] <snek>
how about tryAcquire

[14:22:27.0480] <Luca Casonato>
> <@michaelficarra:matrix.org> and, not or

depends on if the token is a protocol or a class

[14:22:31.0651] <rbuckton>
Well, it's listed as "non-essential", to be fair, but if it did it would pollute semaphore with unrelated functionality, IMO.

[14:22:37.0021] <shu>
like, it would be a blocking void `acquire()` that can't be called on the main thread

[14:22:42.0344] <Luca Casonato>
> <@devsnek:matrix.org> how about tryAcquire

https://github.com/michaelficarra/proposal-concurrency-control/issues/2

[14:23:18.0176] <Michael Ficarra>
> <@devsnek:matrix.org> how about tryAcquire

that was in the post-Stage 1 design considerations slide

[14:23:23.0014] <Michael Ficarra>
sorry it went by really fast

[14:23:33.0573] <snek>
no i was distracted, mb

[14:23:44.0414] <bakkot>
> <@shuyuguo:matrix.org> like, it would be a blocking void `acquire()` that can't be called on the main thread

how attached are you to the `void` part? the fact that you can `release` a thing you didn't previously `acquire` seems like a non-essential (and bad) part of textbook C semaphore implementation

[14:24:31.0205] <rbuckton>
In my `AsyncSemaphore` implementation, you don't `acquire` and then `dispose`, as that grants a capability to the `acquire` caller that belongs on the semaphore itself

[14:24:53.0414] <snek>
i am also a fan of release tokens. you can build the meh c api on top of that if you want.

[14:25:18.0084] <rbuckton>
> <@bakkot:matrix.org> how attached are you to the `void` part? the fact that you can `release` a thing you didn't previously `acquire` seems like a non-essential (and bad) part of textbook C semaphore implementation

Why is this bad? The semaphore maintains its capabilities rather than blindly handing them off to callers.

[14:25:42.0954] <littledan>
another reason why those other snapshots are not all exposed is because these are often not useful and each is complicated to define. So the idea is to add the "causal snapshots" where they are useful.

[14:25:46.0969] <bakkot>
... access to the semaphore should not imply access to the ability to cause the semaphore to think it has more resources available

[14:25:55.0012] <rbuckton>
IMO that's like putting `cancel()` on a `Promise` return value. It's the wrong separation of concerns.

[14:25:58.0008] <bakkot>
you should only be able to cause it to think it has resources if it previously told you that you owned those resources

[14:26:24.0609] <bakkot>
I have exactly the reverse intuition about who should be responsible for this capability

[14:26:51.0855] <shu>
yeah that particular implementation keeps a magic internal _count to determine

[14:26:56.0169] <snek>
wait ron you're arguing that there should be a release method that requires no previously acquired capability?

[14:27:10.0484] <rbuckton>
To use the train or ship "Semaphore" metaphor, that's like waiving a flag to let a user in, and then that user gets to waive the flag to let the next user in. 

[14:27:14.0876] <snek>
 * wait ron you're arguing that there should be a release method that does not require a acquired capability?

[14:27:18.0164] <shu>
https://github.com/dotnet/runtime/blob/4c58b5a5132cb089b23d32cafe3fcfa7e615a0da/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs#L225-L230

[14:27:19.0008] <snek>
 * wait ron you're arguing that there should be a release method that does not require a previously acquired capability?

[14:27:37.0215] <bakkot>
... right, yes, that's the correct thing

[14:27:40.0426] <bakkot>
the user announces when they're done

[14:27:48.0095] <shu>
i am warming up to the idea of letting the VM do complete magic here without a userland hint

[14:27:51.0602] <bakkot>
the user does not get to say "someone is done", only "I personally am done"

[14:27:53.0609] <Luca Casonato>
> <@rbuckton:matrix.org> To use the train or ship "Semaphore" metaphor, that's like waiving a flag to let a user in, and then that user gets to waive the flag to let the next user in.

It's like waving the flag, and then giving the flag to the user. And then when the user is done they have to give the flag back to whoever is waving flags so they can wave in the next person

[14:27:57.0742] <rbuckton>
> <@devsnek:matrix.org> wait ron you're arguing that there should be a release method that does not require a previously acquired capability?

That's how my `AsyncSemaphore` works, though releasing an empty semaphore does nothing.

[14:28:12.0706] <rbuckton>
And I do have "release tokens" in many of the other coordination primitives in esfx

[14:28:25.0387] <snek>
i am new to someone arguing that linear types are bad

[14:28:36.0261] <littledan>
do <2 minutes remain?

[14:28:51.0505] <littledan>
Mark: Yes, that is the case

[14:29:12.0380] <keith_miller>
I guess it's also weird because all of the examples are behaviors that the particular lock implementation wants and don't seem to be architecture specific.

[14:29:12.0896] <ryzokuken>
> <@littledan:matrix.org> do <2 minutes remain?

looked like a TCQ bug

[14:30:10.0789] <keith_miller>
Maybe for compile times? :P

[14:30:21.0015] <snek>
lol, fair

[14:31:08.0074] <shu>
the high-level goal here is for the compilers to do the optimal thing in both the interpreter (where the call overhead is huge) and in inlined code (where you can inline `pause`)

[14:31:18.0038] <shu>
do you see a way to do that other than an implementation-defined hint parameter? that was my best idea

[14:31:28.0282] <Chris de Almeida>
this topic goes to the end of day (~28 mins remaining)

[14:31:39.0093] <shu>
if that causes more confusion and problems, then i will drop it. but that also means giving up on that goal, which is too bad

[14:31:41.0848] <rbuckton>
> <@devsnek:matrix.org> i am new to someone arguing that linear types are bad

In the case of `Semaphore`, it's about capability and separation of concerns.

[14:31:48.0163] <shu>
 * the high-level goal here is for the VMs to do the optimal thing in both the interpreter (where the call overhead is huge) and in inlined code (where you can inline `pause`)

[14:32:38.0670] <keith_miller>
Do you just want consistent timing? Or are you looking for something else? I guess I don't see how you're anticipating the goal to be achieved.

[14:33:10.0566] <rbuckton>
Just because a user can invoke `acquire()` or `wait()` on a semaphore does not mean that the next waiting user should automatically be let in when that user is done. A counting semaphore only lets one user in, but can release `n` waiting users as is necessary.

[14:33:25.0388] <keith_miller>
I don't see how the loop count is related to the JIT tier

[14:33:44.0719] <shu>
i want consistent timing, yes

[14:34:16.0105] <shu>
if i have a fast path loop:

```
do {
  TryLock();
  Atomics.pause();
} while (spins++ < kSpinCount)
```

[14:34:19.0167] <keith_miller>
I guess I still don't see how the loop hint helps with that?

[14:34:32.0978] <shu>
 * if i have a fast path loop:

```
do {
  TryLock();
  Atomics.pause(spins);
} while (spins++ < kSpinCount)
```

[14:35:00.0352] <shu>
what i was going for was, if that do-while loop is running in the interpreter, `Atomics.pause(spins)` would always execute one `pause`

[14:35:01.0594] <keith_miller>
The goal seems reasonable. But wouldn't it just be implemented with extra pauses in the JIT?

[14:35:22.0173] <shu>
if it's inlined JIT code, Atomics.pause(spins) would use `spins` to determine how many `pause`s

[14:35:39.0688] <shu>
> <@keith_miller:matrix.org> The goal seems reasonable. But wouldn't it just be implemented with extra pauses in the JIT?

well right, but i wanted the user to signal that intent instead of doing it for _all_ calls to `pause`

[14:35:50.0795] <shu>
i don't think it's the end of the world to do it for all calls of pause, to be clear

[14:35:54.0824] <shu>
i thought this would be uncontroversial

[14:36:17.0703] <keith_miller>
Are you saying you want the total spin loop time to be the same between JIT and interpreter?

[14:36:30.0754] <shu>
yes

[14:36:40.0754] <keith_miller>
Or that any given pause is the same

[14:36:41.0706] <shu>
i mean, or as close as feasible

[14:36:49.0802] <shu>
not some hard real-time kind of guarantee

[14:36:55.0393] <keith_miller>
I see, I misunderstood

[14:37:18.0719] <shu>
if i write a mutex implementation in JS, i don't want the contention fast path to be waiting for longer/shorter depending on whether you're in the interpreter or in the JITs

[14:38:54.0106] <shu>
there are different ways to accomplish that goal: i thought a hint parameter was the most flexible

[14:40:16.0512] <keith_miller>
Hmm, it almost feels like the right way to express that would be `Atomics.compareExchangeWithRetry(..., count)` or something but idk

[14:40:30.0105] <keith_miller>
Or at least less controversial at this point lol

[14:41:04.0837] <bakkot>
The user should be the one who indicates that they are done. Ability to `acquire` should not give you any further capabilities except the ability to indicate that you're done. And you should not have the ability to indicate that you're done without previously having exactly one corresponding `acquire`.

[14:41:10.0621] <keith_miller>
> <@shuyuguo:matrix.org> if i write a mutex implementation in JS, i don't want the contention fast path to be waiting for longer/shorter depending on whether you're in the interpreter or in the JITs

I kinda agree but isn't that already up to scheduling in native-land anyway?

[14:41:24.0504] <bakkot>
 * The user should be the one who indicates that they are done. Ability to `acquire` should not give you any further capabilities except the ability to indicate that you're done. And you should not have the ability to indicate that you're done without previously having exactly one corresponding completed `acquire`.

[14:41:43.0479] <shu>
right, it's a best effort thing

[14:41:46.0655] <shu>
my thread can certainly be preempted

[14:42:10.0343] <shu>
i mean, that doesn't stop C++ implementations from trying to be clever with inline asm and spin counts even though it's also up to OS scheduling

[14:42:27.0892] <shu>
since it shows enough improvement on enough workloads on enough OSes

[14:43:56.0674] <rbuckton>
It's easy enough for a consumer to wrap release:
```js
class WorkArea {
  #sem = new Semaphore(1);
  async acquire() {
    await this.#sem.acquire();
    const stack = new DisposableStack();
    stack.defer(() => { this.#sem.release(1); });
    return stack;
  }
}
```
But you can also implement semaphores wrappers that defer the actual call to `release()` based on other heuristics, such as application load, quiescence, etc.

[14:44:11.0290] <rbuckton>
 * It's easy enough for a consumer to wrap release:

```js
class WorkArea {
  #sem = new Semaphore(1);
  async acquire() {
    await this.#sem.acquire();
    const stack = new DisposableStack();
    stack.defer(() => { this.#sem.release(1); });
    return stack;
  }
}
```

But you can also implement semaphore wrappers that defer the actual call to `release()` based on other heuristics, such as application load, quiescence, etc.

[14:45:10.0436] <rbuckton>
Counting semaphores, especially, have different cases than a simple binary semaphore or mutex

[14:45:34.0078] <keith_miller>
Yeah, idk, I think if I really cared about this enough I would probably just implement my spin loop in WASM where I have more control on the assembly that's generated anyway.

[14:47:01.0621] <snek>
why would you want to have to write a wrapper for the behavior that enforces correct usage

[14:47:31.0361] <rbuckton>
You're assuming a single correct usage, and there is not a single correct usage.

[14:48:28.0753] <Justin Ridgewell>
> <@rbuckton:matrix.org> You're assuming a single correct usage, and there is not a single correct usage.

I don‚Äôt understand why you‚Äôd encoruage putting `dispose` on the manager instead of the instance.

[14:49:15.0144] <rbuckton>
You can also separate the "locking" mechanism of a semaphore from the "counting" mechanism  of a semaphore, just as we're considering separating the "locking" mechanism of a Mutex from the state of the mutex, e.g.:

```
const mut = new Mutex();
startThread(mut, data);
{
  using lck = new UniqueLock(mut);
  ...
} // releases `lck`
```



[14:49:20.0956] <shu>
but i still have not heard a counter argument on why it's bad to have a hint

[14:49:28.0537] <snek>
i think the release(n) use case is valid and is also better served with tokens.

[14:49:31.0888] <rbuckton>
> <@jridgewell:matrix.org> I don‚Äôt understand why you‚Äôd encoruage putting `dispose` on the manager instead of the instance.

I didn't?

[14:50:03.0321] <Justin Ridgewell>
tauri://localhost/#/%23tc39-space%3Amatrix.org/%23tc39-delegates%3Amatrix.org/%24-8J3nO4bp71-0prpP61GST9bcoyqHtYjRn_JCuXs0NA is on the manager instead of the token instance.

[14:50:14.0117] <rbuckton>
`acquire().dispose()` maybe makes sense for binary semaphores, but not necessarily counting semaphores.

[14:50:17.0128] <Justin Ridgewell>
Wow, that‚Äô copy-link is borked.

[14:50:33.0333] <rbuckton>
That link didn't work in matrix

[14:50:47.0983] <Justin Ridgewell>
https://matrixlogs.bakkot.com/TC39_Delegates/2024-07-29#L327

[14:51:03.0768] <bakkot>
In JavaScript, I claim, the overwhelming most common use case for an API that looks kinda like this is "I want to make at most 5 simultaneous network requests" (or database connections, or filesystem accesses, or threads, or whatever). Leaving aside naming questions, I think that use case is best served by the API presented today.

[14:51:06.0647] <Justin Ridgewell>
Cinny apparently doesn‚Äôt implement link to message well.

[14:51:45.0635] <snek>
i don't agree that there is a difference in api preference for n=1 vs n>1

[14:51:57.0893] <rbuckton>
> <@jridgewell:matrix.org> https://matrixlogs.bakkot.com/TC39_Delegates/2024-07-29#L327

That example didn't put dispose on the manager? It wrapped `release` in the call to `acquire` to emulate what snek was suggesting.

[14:52:19.0465] <bakkot>
If the concern is "I sometimes want to do something other than that, and I think of that other thing as being called Semaphore", that's a fair piece of feedback to support the rename, but not to support a different API.

[14:52:22.0244] <Justin Ridgewell>
`this.#sem.release(1)`, with `#sem` being a manger.

[14:52:50.0083] <rbuckton>
That's what a `{ acquire(): Promise<Disposable> }` would do anyways?

[14:53:54.0767] <kriskowal>
So, consider `ReverseAsyncContext` with `add` and `reduce` instead of `set` and `get`.

[14:54:24.0737] <snek>
what does this mean

[14:54:36.0072] <keith_miller>
If the hint does anything other than monotonically increase/decrease doesn't it kinda need to know the limit anyway?

[14:55:05.0073] <keith_miller>
I don't think it's necessarily bad just that without any comment on what it means then it's unclear.

[14:55:07.0148] <kriskowal>
We were looking into something like this for opentracing, opencontext, which is now opentel.

[14:55:33.0221] <snek>
oh no i just mean what is a ReverseAsyncContext

[14:55:36.0573] <kriskowal>
We were interested because it would make it possible to solve some ‚Äúread after write‚Äù hazards, with a ‚Äúcookie‚Äù or lamport clock variants

[14:55:41.0870] <shu>
there would be an internal limit

[14:55:56.0595] <kriskowal>
It‚Äôs AsyncContext but the variables are sets instead of cells.

[14:55:59.0210] <shu>
like no matter waht the input is you'd never `pause` more than N times, whatever that is

[14:56:03.0409] <shu>
50, 100

[14:56:06.0243] <keith_miller>
Like what if I like to write my loops as `for (I = spinCount; I--;) Atomics.yield(i);`

[14:56:17.0057] <kriskowal>
Such that it‚Äôs meaningful for information to flow up the stack instead of broadcast outward.

[14:56:18.0673] <keith_miller>
because it's less characters or w/e

[14:56:24.0468] <snek>
oh i see, got it

[14:56:44.0886] <snek>
 * oh i see, got it, ty

[14:56:57.0796] <keith_miller>
Wait then why not have the JIT inject a count into the IR?

[14:57:32.0218] <kriskowal>
The reducer would merge all the members of the set such that the next consumer would have less work.

[14:58:08.0192] <kriskowal>
It is probably a Bad Idea‚Ñ¢ but this is the natural conclusion. There was a paper about it that I will have to find.

[14:59:33.0716] <Chengzhong Wu>
Thanks for sharing! We was discussing about possible merging/reducing with this flow at https://github.com/tc39/proposal-async-context/pull/94#discussion_r1651720741 

[15:00:15.0525] <shu>
> <@keith_miller:matrix.org> Like what if I like to write my loops as `for (I = spinCount; I--;) Atomics.yield(i);`

yeah, that's fair

[15:00:20.0948] <littledan>
yeah we were thinking that it'd be bad if JS code got injected into the middle whenever context merges happen (which is all the time)

[15:01:09.0096] <rbuckton>
> <@bakkot:matrix.org> If the concern is "I sometimes want to do something other than that, and I think of that other thing as being called Semaphore", that's a fair piece of feedback to support the rename, but not to support a different API.

I have two concerns:
1. The `Semaphore` API proposed depends on asynchrony and is geared for async coordination, not thread coordination. We also hope to introduce other thread coordination primitives in the future, including a `Semaphore` in line with `Mutex` and `Condition` that can be shared and does not require asynchrony. As such, we must avoid conflicting naming.
2. Before we decide on a final API for an async coordination `Semaphore`, we must ensure we understand the full problem domain of counting semaphores, to ensure we chose an API that is consistent with intended use cases.

[15:01:23.0043] <shu>
the guidance is that you pass monotonic increasing integers as hints that you are pausing for the Nth time

[15:01:35.0486] <shu>
whatever strategy the VM chooses, it needs to understand that

[15:01:36.0328] <kriskowal>
Coming to think of it, putting a Set in an AsyncContext variable can‚Äôt be prevented, and this kind of work will be possible regardleess

[15:01:38.0845] <shu>
it can then choose whatever

[15:02:08.0693] <kriskowal>
And it‚Äôs only safe anyway if the mergers are consistent (which is enforced to a degree by ocap discipline around holding the variable)

[15:02:58.0006] <rbuckton>
For `Mutex`, we have been discussing an API design to allow `Mutex`, `Condition`, and `UniqueLock` to work in a blocking manner (when off the UI thread in the browser), and an async manner (when on the UI thread in the browser). The current design has both sync and async methods on `UniqueLock`.

[15:03:54.0071] <kriskowal>
Ah, but contexts derived from a context holding growing subsets that might contribute to the parent set is not there.

[15:04:05.0011] <rbuckton>
Were we able to consider `Semaphore` for the MVP for shared structs, it likely would be a sharable object like `Mutex`, and have both a blocking (off UI thread) and non-blocking async (on UI thread) API, that would likely make a separate `Semaphore` unnecessary.

[15:04:06.0446] <snek>
why do you draw a line between binary and counting semaphores for this api? 

[15:04:13.0540] <snek>
do you think they must have different apis?

[15:04:17.0633] <kriskowal>
Well, it‚Äôll be fun to talk about.

[15:05:11.0713] <kriskowal>
(I for one do not think that we need to replay the history of mutual exclusion with all its conventional names.)

[15:05:22.0706] <rbuckton>
Not necessarily. But I'm not sold on `acquire().dispose()`. I can see it making sense on a binary semaphore, as it can only ever have one user, but a counting semaphore can have multiple users, or release in batches.

[15:06:53.0427] <kriskowal>
((I also for one do not think that we need to foist the unavoidable downsides of mutual exclusion on JavaScript, which is more useful for some workloads due to omission while not precluding folks from using other languages for workloads for which they are better suited.))

[15:07:36.0954] <shu>
keith_miller msaboff waldemar rbuckton https://github.com/tc39/proposal-atomics-microwait/issues/9

[15:12:55.0447] <bakkot>
> <@rbuckton:matrix.org> Were we able to consider `Semaphore` for the MVP for shared structs, it likely would be a sharable object like `Mutex`, and have both a blocking (off UI thread) and non-blocking async (on UI thread) API, that would likely make a separate `Semaphore` unnecessary.

`Mutex` does not handle the "I want to make at most 5 simultaneous network requests" case, which is (I claim) the most common case for something like this in JS. But if you just change the binary to counting, that's just what's proposed here, plus natural extensions already discussed: an off-thread-only `acquireSync` plus coordination with the host to be `structuredClone`-able.

[15:13:15.0402] <bakkot>
> <@rbuckton:matrix.org> Were we able to consider `Semaphore` for the MVP for shared structs, it likely would be a sharable object like `Mutex`, and have both a blocking (off UI thread) and non-blocking async (on UI thread) API, that would likely make a separate `Semaphore` unnecessary.

 * `Mutex` does not handle the "I want to make at most 5 simultaneous network requests" case, which is (I claim) the most common case for something like this in JS. But if you change the binary nature of `Mutex` to be counting instead, that's just what's proposed here, plus natural extensions already discussed: an off-thread-only `acquireSync` plus coordination with the host to be `structuredClone`-able.

[15:13:54.0244] <rbuckton>
> <@devsnek:matrix.org> why do you draw a line between binary and counting semaphores for this api?

Also, let me back up a bit and clarify my position. I have a negative gut reaction to `acquire().dispose()` possibly due to my experience and prior use of semaphores in .NET and C++, which both keep the acquire/release capabilities on the semaphore itself. To address that reaction, I need additional time to consider the implications of such an API over the existing use cases. Normally I would be chomping at the bit for the DX improvement that `acquire().dispose()` might provide, but the shared structs proposal is trying to enable a very specific set of capabilities that have unique concerns that differ from most existing JS code. 

[15:14:16.0504] <keith_miller>
Hmm, I guess, I'm a bit confused about the objection to monotonically increasing pauses? Isn't that morally the same as what you proposed today? If you want shorter and shorter waits just write your loop as a count down? What changed that made you think monotonic was a problem?

[15:16:20.0284] <rbuckton>
I also regret I did not have the time to flesh out those concerns prior to the meeting, unfortunately a week was not long enough given other pressing concerns. I hope to have a longer discussion with Michael Ficarra following the plenary.

[15:16:51.0741] <keith_miller>
 * Hmm, I guess, I'm a bit confused about the objection to monotonically increasing pauses? Isn't that morally the same as what you proposed today? If you want shorter and shorter waits just write your loop as a count down. We could even add such a note to MDN. What changed that made you think monotonic was a problem?

[15:17:04.0248] <keith_miller>
 * Hmm, I guess, I'm a bit confused about the objection to monotonically increasing pauses? Isn't that morally the same as what you proposed today? If you want shorter and shorter waits just write your loop as a count down. We could even add such a note to the spec or MDN. What changed that made you think monotonic was a problem?

[15:17:10.0219] <snek>
i see. my personal experience using counting and binary semaphores in rust (which is just tokio::sync::Semaphore, its the same api) is that it works well for both cases, and it has helped to reduce bugs in more complex use cases like the ones you seem to be concerned about.

[15:17:33.0858] <shu>
i probably overreacted

[15:17:40.0829] <shu>
i am not really objected to monotonic increasing

[15:18:06.0997] <shu>
i think it is slightly less ergonomic because i think monotonic decreasing wait time is the right default interpretation

[15:18:13.0456] <shu>
but as you say i can of course just write the opposite loop

[15:18:54.0439] <shu>
but it's easy to flip that around: why is monotonic increasing the right default?

[15:19:15.0070] <keith_miller>
> <@shuyuguo:matrix.org> i think it is slightly less ergonomic because i think monotonic decreasing wait time is the right default interpretation

I guess I disagree with that assumption but I don't have any data to back it up.

[15:19:29.0745] <shu>
yeah, tbf neither do i

[15:19:36.0240] <rbuckton>
> <@devsnek:matrix.org> i see. my personal experience using counting and binary semaphores in rust (which is just tokio::sync::Semaphore, its the same api) is that it works well for both cases, and it has helped to reduce bugs in more complex use cases like the ones you seem to be concerned about.

That's well and good, I'm just coming at `Semaphore` from a different perspective. I also was a bit put off by the, albeit optional, `wrap` and `wrapIterator` convenience methods, they felt out of place and seemed to be pushing `Semaphore` towards being very specific to async iteration use cases.

[15:19:41.0227] <keith_miller>
In other contexts backoff is typically longer and longer I guess

[15:19:41.0811] <shu>
the only tie-breaking data i see is there might be non-monotonic relationships that are reasonable as well

[15:20:09.0876] <shu>
(and like, there's no way to check for non-compliance, so whatever relationship we _say_ in that step, an implementation can still do anything it wants)

[15:20:35.0981] <shu>
so given that, i am also happy to say that the hint is monotonic increasing

[15:21:04.0043] <keith_miller>
But non-monotonic seems like it would need to know the upper bound in most cases.

[15:21:13.0010] <shu>
so does monotonic

[15:21:20.0443] <shu>
you don't want a user to be able to `pause` for 2^53 times...

[15:21:35.0677] <keith_miller>
Well that would presumably a static cap?

[15:21:40.0034] <shu>
right

[15:21:44.0615] <keith_miller>
 * Well that would presumably be a static cap?

[15:21:57.0640] <bakkot>
(Those would be on Governor, not Semaphore, and would be implemented purely in terms of `acquire`/`release`. also `wrap` is useful for lots of stuff; it's basically https://www.npmjs.com/package/throat which gets used a bunch)

[15:22:22.0021] <keith_miller>
But e.g. if you wanted to do X -> X-1 -> X-2 -> X-1 -> X you need to know the midpoint

[15:22:42.0149] <rbuckton>
From working with `Atomics.Mutex` in the Shared Structs dev trial, and experimenting with a `UniqueLock` approach that dovetailed with `using`, I found I quite liked way concerns were separated. In that design, `Mutex` is essentially an opaque shared token. To take, attempt to take, release, or assume a lock required a non-shared `UniqueLock` wrapper that was bound to the local scope.

[15:22:55.0479] <keith_miller>
I dunno, I guess this seems like way too deep into the bike shed lol

[15:23:08.0814] <keith_miller>
 * I dunno, maybe we're way too deep into the bike shed lol

[15:23:29.0518] <rbuckton>
Were we to do `acquire().release()` for `Semaphore`, I would have generally preferred to use a similar API design to `Mutex`/`UniqueLock`.

[15:23:38.0723] <rbuckton>
Pardon, I must break for dinner. I will return shortly.

[15:25:02.0316] <keith_miller>
I guess if I were reading the API I would like to have some intuition about how the VM should/might be using my hint that doesn't involve reading the implementation's source (if it's even available).

[15:25:30.0254] <keith_miller>
Especially for such a low level API

[15:26:06.0632] <keith_miller>
Anyway, I'll move this to the GH issue since that seems like the right place for this discussion at this point

[15:27:43.0822] <shu>
i think what's crystallizing for me is that i am fine with dropping the complete implementation-defined language, because since it's timing only, you can do anything you want as an as-if implementation

[15:27:54.0952] <shu>
so _any_ language about how _iterationNumber_ is used i'm fine with

[15:27:58.0857] <shu>
we can say it's monotonic increasing

[15:28:15.0131] <shu>
i'd prefer that to dropping the hint parameter entirely

[15:30:14.0211] <waldemar>
> <@jridgewell:matrix.org> Would Waldemar be satisifed with bigger-number-longer-wait? Then let the engine handle that to match the VM‚Äôs CPU instructions, eg smaller-number-longer-wait?

That's how it already is. I'd like to know more about the relationship between the number and the wait time. As far as I can tell, Shu's bigger-number-shorter-wait was not in the spec and I don't know why he made that claim at the meeting.

[15:32:35.0088] <shu>
it was in the spec, but i take the point the wording was confusing to many and not well communicated in previous meetings

[15:34:17.0883] <shu>
waldemar: after chatting with keith_miller i think i am perfectly happy with spelling out the bigger-number-longer-wait semantics. but i'd like to caution that since timing is not observable behavior, an implementation can still choose another interpretation of the hint as an as-if optimization.

would spelling out bigger-number-longer-wait satisfy your concerns?

[15:39:09.0638] <waldemar>
> <@shuyuguo:matrix.org> waldemar: after chatting with keith_miller i think i am perfectly happy with spelling out the bigger-number-longer-wait semantics. but i'd like to caution that since timing is not observable behavior, an implementation can still choose another interpretation of the hint as an as-if optimization.
> 
> would spelling out bigger-number-longer-wait satisfy your concerns?

It already spells out bigger-number-longer-wait. For example, "The number of times the signal is sent for an integral Number N is at most the number of times it is sent for N + 1." and "For integral numbers N, Atomics.pause(N) should wait at most as long as Atomics.pause(N+1)". I was really confused by your claim that it does bigger-number-shorter-wait.

[15:40:14.0781] <waldemar>
And yes, I'd like to keep bigger-number-longer-wait but reword the note that states that the programmer should increase N linearly. What the programmer does with N is up to them.

[16:08:00.0818] <rbuckton>
.NET's `Thread.SpinWait()` (the equivalent of `Atomics.pause()`) describes itself as "a busy wait in a very tight loop that spins for the number of iterations specified", though this isn't 100% accurate, it's fairly close. It uses some heuristics to normalize processor "yield" instructions, when in turn correlates to platform/architecture equivalent ASM "pause" or "yield" instructions.
.NEt'

[16:08:05.0564] <rbuckton>
 * .NET's `Thread.SpinWait()` (the equivalent of `Atomics.pause()`) describes itself as "a busy wait in a very tight loop that spins for the number of iterations specified", though this isn't 100% accurate, it's fairly close. It uses some heuristics to normalize processor "yield" instructions, when in turn correlates to platform/architecture equivalent ASM "pause" or "yield" instructions.

[16:09:06.0217] <rbuckton>
.NET's `SpinWait` struct uses `Thread.SpinWait()` under the hood, but maintains a local counter which does the more complex backoff mechanism.

[16:10:03.0861] <shu>
bakkot: i did not have the capacity to fully pay attention to the governor thing and the Atomics.pause discussion, but a fully generic "unlock token" concept seems elegant. but the gut reaction i have is (wait for it) performance concerns about the allocations of these tokens in hot paths -- ideally i'd like to not bottleneck finer grained locking architectures

[16:10:36.0455] <shu>
rbuckton: i think i've been convinced on "large N is longer wait" is just fine to spec, see the PR https://github.com/tc39/proposal-atomics-microwait/pull/11

[16:10:40.0409] <rbuckton>
e.g., `spinWait.SpinOnce(sleep1Threshold)` increments the internal spin counter and uses the provided argument to indicate how often to back off to a full `Thread.Sleep(1)`.

[16:11:38.0065] <shu>
the user code can pass in linearly increasing N if they want linear backoff, exponentially increasing N if they exponential backoff, or the reverse if they want "smaller N longer wait" semantics like i originally envisioned. the JIT and interpreter can scale the wait time differently

[16:11:47.0621] <rbuckton>
Something like the `SpinWait` struct can be implemented in userland to introduce a backoff mechanism, so long as we have `Atomics.pause(iterations)` to trigger a busy loop w/o sleeping, which aligns with "large N is longer wait"

[16:11:58.0232] <bakkot>
> <@shuyuguo:matrix.org> bakkot: i did not have the capacity to fully pay attention to the governor thing and the Atomics.pause discussion, but a fully generic "unlock token" concept seems elegant. but the gut reaction i have is (wait for it) performance concerns about the allocations of these tokens in hot paths -- ideally i'd like to not bottleneck finer grained locking architectures

I think that having `semaphore.acquireAndCall(fn)` for the hot paths would make sense. though I'd guess in hot paths the token objects would never leave the nursery and therefore(?) not be that expensive?

[16:12:29.0535] <rbuckton>
I think I was conflating how `SpinWait` the struct and `Thread.SpinWait()` work in my responses.

[16:13:26.0951] <shu>
> <@bakkot:matrix.org> I think that having `semaphore.acquireAndCall(fn)` for the hot paths would make sense. though I'd guess in hot paths the token objects would never leave the nursery and therefore(?) not be that expensive?

one can hope hot path means critical section = very short, yes

[16:13:56.0067] <bakkot>
well, either it's short or infrequent, can't be both long and frequent

[16:14:00.0017] <shu>
and if your GC has per-thread linear allocation buffers for shared objects, then hopefully also okay

[16:14:11.0547] <bakkot>
the tokens need not be shareable

[16:14:15.0060] <bakkot>
though they could be I guess

[16:14:24.0193] <shu>
oh, these are sync tokens?

[16:14:33.0673] <shu>
i'm not clear on what's sync and what's async

[16:14:59.0825] <bakkot>
design is the same either way

[16:15:26.0885] <bakkot>
that is, it's either `acquireAsync: Promise<token>` or `acquireSync: token`, but in either case you have a sync `token.release()`

[16:15:52.0108] <bakkot>
(by "either" I mean "my ideal `Semaphore` would have both, with the sync one only available off main thread)

[16:15:59.0226] <bakkot>
 * (by "either" I mean that my ideal `Semaphore` would have both, with the sync one only available off main thread)

[16:16:42.0025] <bakkot>
but you are not generally going to need to send the tokens themselves across threads, except in very rare cases which I don't know if we'd need to support if it's expensive to do so

[16:17:08.0360] <bakkot>
so the tokens would not need to be shared objects

[16:17:27.0957] <bakkot>
would be kind of nice, if it was easy, but it's certainly not necessary

[16:17:59.0992] <shu>
but the decision on whether the token is shareable needs to be a priori

[16:18:08.0368] <shu>
unless `token` above was shorthand for 2 different types

[16:18:16.0921] <shu>
`Promise<shareable token>` and `sync token`

[16:20:28.0374] <bakkot>
it could also be cloneable-but-not-shareable

[16:20:36.0261] <shu>
yeah

[16:20:42.0393] <bakkot>
which is perhaps the best of all worlds here

[16:21:09.0897] <shu>
anyway this deserves more of my time and i'm hesitant to give any preferences until i've had time to properly digest

[16:21:23.0024] <bakkot>
yeah

[16:21:47.0690] <bakkot>
this affects mutexes too but presumably there's still plenty of time for revisions to the shared structs proposal?

[16:22:06.0738] <shu>
yes, i'd say definitely true for synchronization primitives 

[16:23:17.0818] <bakkot>
for which, https://github.com/tc39/proposal-structs/issues/27


2024-07-30
[09:59:54.0773] <ljharb>
is there any way we can get the captioner to hit enter less often?

[10:00:09.0252] <littledan>
we should ask them to do so

[10:00:14.0255] <littledan>
others in their team know how to do it

[10:00:27.0057] <littledan>
just interrupt and tell them, please copy the settings from your colleagues who do this right

[10:00:38.0084] <littledan>
as well as any other quality issues, we should just note it to them

[10:06:09.0933] <nicolo-ribaudo>
Fyi, Webex has a "zoom in button" which works quite well

[10:10:06.0908] <kriskowal>
I got a lead on a paper that purportedly discusses context joins but I‚Äôve not yet found the bit I remember about deferring ‚Äúbaggage‚Äù merge to the point of consumption. Still, it‚Äôs pertinent to async context motivating use cases  https://cs.brown.edu/~jcmace/mace_thesis.pdf

[10:12:42.0031] <kriskowal>
I got this from the proverbial horse‚Äôs mouth. I reached out to Yuri Shkuro in the Cloud Native Foundation‚Äôs Slack.

[10:14:35.0672] <kriskowal>
In any case, I think reverse context propagation is probably a non-starter just because it implies that parent contexts will have to at least weakly retain child contexts, or something that has a similar retention graph, regardless of any as-yet-unsought implications for security.

[10:21:26.0921] <shu>
Chris de Almeida: seems like there's a lot of free time this meeting. can i request a 30min continuation of Atomics.pause discussion if it doesn't spill to day 4?

[10:27:40.0287] <Chris de Almeida>
shu: do you want 30 mins today before lunch?

[10:28:01.0721] <shu>
i'd prefer to not bump anyone else already lined up and do it at the end

[10:28:11.0397] <shu>
but will defer to chairs' judgment for the best schedule

[10:28:15.0246] <Chris de Almeida>
it's not quite bumping folks

[10:28:16.0322] <Chris de Almeida>
we got back time from the last 2 topics going quickly

[10:28:41.0695] <shu>
i'd still prefer we try to move other already scheduled topics up first

[10:28:46.0307] <shu>
if no other takers i'll take it

[10:28:47.0861] <Chris de Almeida>
ok

[10:40:32.0218] <Justin Ridgewell>
Can we no longer reply to the active TCQ topic?

[10:41:08.0764] <Chris de Almeida>
you can

[10:41:47.0109] <ryzokuken>
you can't

[10:41:52.0585] <ryzokuken>
because its a clarifying question itself

[10:41:54.0145] <Chris de Almeida>
we are currently on a clarifying question item

[10:42:01.0368] <Chris de Almeida>
what?

[10:42:15.0116] <ryzokuken>
yeah it's not a full topic 

[10:42:39.0921] <Chris de Almeida>
yes, you can't reply to a clarifying question, once we advance past clarifying questions, the button you want will return

[10:43:08.0806] <Chris de Almeida>
if you have a reply to this topic, use clarifying question I guess?

[10:43:26.0861] <ljharb>
(that's what i did)

[10:45:07.0367] <bakkot>
tcq feature request: convert the current item to a real topic

[10:45:14.0607] <bakkot>
or just allow replying to clarifying questions, I guess

[10:49:55.0135] <Aki>
Frank isn't on Matrix, right?

[10:50:25.0453] <ryzokuken>
apparently not

[10:50:59.0349] <Aki>
The notes lack a summary/conclusion for his topic, would someone please volunteer to either get him to write it (if you have a way of being in contact) or writing it up?

[10:51:45.0283] <Chris de Almeida>
ah, we forgot to ask I think

[10:52:08.0298] <Chris de Almeida>
and the earlier topic as well

[10:53:05.0813] <Aki>
yeah i already messaged Michael to take a look after this topic

[10:54:02.0712] <ryzokuken>
thanks for the reminder I'll ping Frank elsewhere

[10:54:25.0292] <Aki>
hat tip to saminahusain for reminding me. it's like an og phone tree ^_^

[10:54:57.0549] <Aki>
(i bet half of you are too young to remember phone trees. i'll :redirect: to tdz)

[11:01:37.0304] <saminahusain>
üëçÔ∏è

[11:14:27.0322] <Aki>
Don't forget summary/conclusions!

[11:18:33.0130] <littledan>
peetk: would you be up for doing the proposal scrub?

[11:18:50.0911] <peetk>
sure

[11:19:39.0715] <rbuckton>
would like to point out that I will not be available for the rest of TC39

[11:20:33.0481] <rbuckton>
I would have been available on Day 4, but will not be available after the lunch break today or the whole of tomorrow.

[11:20:49.0311] <littledan>
if we could do the proposal scrub some time today, that'd be really great, especially if we'll have tomorrow afternoon to cover other topics

[11:22:06.0666] <rbuckton>
So if we are not getting the the proposal scrub while I am present, I would ask that we postpone any decisions on proposals I am championing that are at stage 2. At the very least, any proposal at stage 2 that I am currently championing should be considered active, if lower priority.

[11:22:24.0525] <rbuckton>
 * So if we are not getting the the proposal scrub while I am present, I would ask that we postpone any decisions on proposals I am championing that are at stage 2. At the very least, any proposal at stage 2 that I am currently championing should be considered active, if lower on my current set of priorities.

[11:22:46.0512] <rbuckton>
 * So if we are not getting to the proposal scrub while I am present, I would ask that we postpone any decisions on proposals I am championing that are at stage 2. At the very least, any proposal at stage 2 that I am currently championing should be considered active, if lower on my current set of priorities.

[11:23:09.0178] <kriskowal>
https://en.wikipedia.org/wiki/Head-of-line_blocking

[11:26:44.0680] <Ashley Claymore>
> <@rbuckton:matrix.org> So if we are not getting to the proposal scrub while I am present, I would ask that we postpone any decisions on proposals I am championing that are at stage 2. At the very least, any proposal at stage 2 that I am currently championing should be considered active, if lower on my current set of priorities.

When we've done scrubs in the past I think this is what we have tried to do. Skip past items if the champion is not present and no one else has up-to-date info on their behalf. I don't think we default to asking for consensus to withdraw.

[11:27:07.0503] <littledan>
yeah the scrub is not about making decisions, just bringing topics up to try to stimulate future discussion.

[11:27:14.0467] <littledan>
Any proposal to withdraw would have to be a separate agenda item

[11:31:45.0834] <littledan>
shu: I don't understand this characterization (which you've made before) about how TC39 just ships stuff and then sees what happens. This is what the stage process, and the various pre-Stage 3 polyfills and transpiler implementations (and sometimes engine implementations), are generally for. I think this has been working well. If you have concerns about this flow of prototyping and feedback, let's have some kind of discussion on it.

[11:32:23.0136] <littledan>
(I don't want to get this discussion into too much of a tangent though)

[11:32:38.0634] <littledan>
we made certain mistakes in the past but I feel like we can learn from those within our current process and improve

[11:32:54.0754] <shu>
it is not my experience that we get any signal from pre-stage 3 polyfills that affects design...?

[11:33:09.0057] <shu>
we don't really have a real incubation process

[11:33:16.0574] <shu>
i don't see how the staging process is one

[11:33:30.0117] <bakkot>
yeah we definitely don't actually get much feedback prior to engines

[11:33:32.0271] <bakkot>
emperically

[11:33:39.0126] <bakkot>
it would be nice if we did but it's hard to do

[11:35:22.0638] <littledan>
what kind of feedback should we be collecting that we're not?

[11:35:36.0089] <bakkot>
a bunch of people using the API and telling us how it went

[11:35:44.0774] <littledan>
are there other examples of more successful bodies who do manage to do this?

[11:35:59.0952] <shu>
WICG?

[11:36:03.0808] <bakkot>
java ships things as experimental for a very long time

[11:36:07.0922] <shu>
like, OTs exist for this reason

[11:36:09.0036] <kriskowal>
_toSorted()_!?

[11:36:14.0792] <shu>
though that's chrome-specific 

[11:37:34.0118] <littledan>
many TC39 features don't really need OTs or shipping as experimental--we can encourage trials via polyfills and transpilers. I guess we haven't been good about making sure we collect feedback when those are out there?

[11:38:34.0269] <bakkot>
also C++ often just standardizes stuff from boost, which is a similar deal

[11:39:01.0431] <shu>
> <@littledan:matrix.org> many TC39 features don't really need OTs or shipping as experimental--we can encourage trials via polyfills and transpilers. I guess we haven't been good about making sure we collect feedback when those are out there?

it is not the state of the world today that we have any "real world" feedback in the design cycle before shipping

[11:39:12.0327] <shu>
we rely on the makeup of delegates to be a good proxy

[11:46:17.0784] <dminor>
Could someone please point me to the html integration issue/PR for Error.isError?

[11:46:39.0746] <littledan>
+1 to non-piercing

[11:46:45.0106] <bakkot>
array template objects are frozen so there's not really any reason to proxy them

[11:47:03.0080] <bakkot>
the proxy can't behave any differently from the underlying thing, except for `===` and weakmaps and so on

[11:47:16.0319] <bakkot>
 * template literal array objects are frozen so there's not really any reason to proxy them

[11:47:18.0287] <littledan>
> <@bakkot:matrix.org> also C++ often just standardizes stuff from boost, which is a similar deal

my understanding is that they worked through most of boost, and also their designs are significantly different so it's like saying that Temporal is based on Moment's real-world experience

[11:47:32.0919] <dminor>
Found it, https://github.com/whatwg/webidl/pull/1421

[11:48:19.0104] <bakkot>
that's fair, I haven't been following them closely for the last decade or so

[11:48:26.0676] <bakkot>
it used to be the case that they'd lift things wholesale, I think

[11:49:08.0716] <littledan>
yeah a decade ago was sort of the heyday of that, but then boost slowed down. it was related to the lack of movement in WG21 in the preceding decade, also.

[11:50:15.0736] <waldemar>
There are a bunch of places where one can find exact, correctly-rounded implementations of the trig and exponential functions on doubles. Here's one: https://core-math.gitlabpages.inria.fr/

[11:50:43.0990] <littledan>
I'm curious if there's a good example of a library-type web API where we can look back on their pre-shipping "real world" feedback and say it's a good model to follow. That could help us.

[11:52:31.0602] <Michael Ficarra>
> <@waldemarh:matrix.org> There are a bunch of places where one can find exact, correctly-rounded implementations of the trig and exponential functions on doubles. Here's one: https://core-math.gitlabpages.inria.fr/

yes, that was one of the implementations mentioned by @sunfishcode:matrix.org in https://github.com/tc39/ecma262/issues/3347#issuecomment-2161705091

[13:00:20.0971] <Ben>
microphhone problems again, I'll volunteer though

[13:01:12.0082] <saminahusain>
TC39 Hats available 

[13:02:12.0849] <Ashley Claymore>
I can do the first 30 mins

[13:02:23.0793] <Ashley Claymore>
but my topic is up after that

[13:07:40.0919] <Ben>
 * microphone problems again, I'll volunteer though

[13:14:13.0323] <Chris de Almeida>
http://ptomato.name/talks/tc39-2024-07

[13:14:19.0394] <Chris de Almeida>
Temporal slides

[13:26:48.0252] <nicolo-ribaudo>
Thanks ptomato for teaching us all those trivia about weird date edge cases btw, they are good conversation topics :)

[13:28:24.0485] <snek>
classic icebreaker: what's your favorite calendar edge case

[13:42:13.0074] <bakkot>
previous discussion of the current topic is in https://github.com/tc39/notes/blob/main/meetings/2024-02/feb-6.md#joint-iteration-for-stage-2

[13:53:20.0120] <Chris de Almeida>
I could've sworn we talked about TZ canonicalization more recently than that?

[13:54:41.0175] <Michael Ficarra>
@peetk:matrix.org proposals repo doesn't have good last-presented dates, use https://github.com/tc39/dataset for that next time

[13:57:27.0882] <ptomato>
2023-07 https://github.com/tc39/notes/blob/main/meetings/2023-07/july-12.md#time-zone-canonicalization-for-stage-3

[13:59:04.0705] <ljharb>
proposals list is updated

[14:02:59.0135] <Michael Ficarra>
I think there's a big disconnect between how excited the community is for the pipeline operator and how excited the committee is

[14:03:04.0306] <ptomato>
I personally don't think the pipeline operator is worth it but that's a weakly held opinion and I'm not speaking on behalf of Igalia

[14:03:17.0690] <Michael Ficarra>
@pchimento:igalia.com same

[14:04:47.0408] <bakkot>
fwiw pipeline is also the proposal that I hear the most _negative_ sentiment about, in its current iteration

