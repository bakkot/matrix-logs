2023-11-01
[18:00:02.0504] <snek>
i think torque could support suspend points natively with a desugaring

[18:00:27.0639] <snek>
this was one of the things i had in mind when i originally suggested the builtin async functions/generators

[18:40:29.0736] <shu>
by all means, i'd love someone to implement it

[18:40:40.0476] <shu>
 * by all means, i'd love for someone to implement it

[23:30:07.0012] <Ashley Claymore>
If anyone wants to give feedback aheadÂ of the 'state of JS' survey: https://github.com/Devographics/surveys/issues/224 

[02:13:41.0650] <yulia>
How long has test262.report been down for? are there plans to bring it back up?

[03:54:00.0400] <nicolo-ribaudo>
test262.fyi :)

[03:55:17.0593] <yulia>
Aaaahhh

[08:47:06.0769] <bakkot>
base64 proposal is updated to remove streaming, fill out spec text, and settle outstanding questions about the API: https://github.com/tc39/proposal-arraybuffer-base64

see playground for overview and a polyfill in the console: https://tc39.es/proposal-arraybuffer-base64/

other than removing streaming, the most significant change is that the base64 decoder is now permissive by default (ignores whitespace, does not require padding), with a `strict: true` options bag argument which makes it strict (does not allow whitespace, enforces padding)

would appreciate eyes on it, and especially if anyone wants to volunteer to review before the November plenary so I can ask for stage 3 (assuming Peter no longer objects); otherwise I'll be asking in January

[09:09:19.0031] <ljharb>
Iâ€™ll volunteer to review


2023-11-02
[10:39:25.0273] <nicolo-ribaudo>
Now that we have dates & time zones for the next meetings, could we add them to the calendar?

[10:39:26.0580] <nicolo-ribaudo>
(or I can do it by myself but I don't know how to add events to the calendar)

[14:18:56.0001] <ljharb>
i'll make sure they're on there


2023-11-07
[23:04:55.0799] <bakkot>
openai announced a new Whisper model, but still no streaming support :(

all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.

whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).

I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)

I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.

anyone happen to have played with any other promising real-time transcription services recently?

[03:03:37.0883] <littledan>
> <@bakkot:matrix.org> openai announced a new Whisper model, but still no streaming support :(
> 
> all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.
> 
> whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).
> 
> I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)
> 
> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.
> 
> anyone happen to have played with any other promising real-time transcription services recently?

saminahusain: 

[03:04:04.0429] <littledan>
Thanks for the report, bakkot . Let's check up on this again at the end of next year.

[03:04:16.0687] <littledan>
sounds like we need to repeat the budget request for transcriptionists

[03:04:33.0931] <littledan>
Are you saying we get good accuracy with a 40-second delay?

[03:04:58.0007] <ryzokuken>
it would be accurate, yeah IIUC

[03:05:17.0188] <ryzokuken>
the 40 second delay is whisper's only shortcoming

[03:06:32.0966] <ryzokuken>
actually, I haven't tried it myself. Wonder how well it does with various accents

[03:07:44.0331] <ryzokuken>
they have an example with a pretty thick accent though, fun

[03:07:45.0834] <ryzokuken>
https://openai.com/research/whisper

[07:11:54.0820] <bakkot>
right. Whisper is very accurate in my tests, but it fundamentally operates on 30-second chunks of audio and takes a little while to run (say 10 seconds per chunk), so trying to stream it to the notes doc would mean that every 30 seconds we get a high-quality transcript of the portion of the meeting starting 40 seconds ago and running through 10 seconds ago. I haven't actually set that up but I expect it to work.

[07:12:19.0097] <bakkot>
unfortunately 40 seconds of lag is a lot of lag

[07:25:11.0927] <Michael Ficarra>
> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.

That would be *so* helpful!

[07:45:46.0531] <Michael Ficarra>
bakkot: What will this cost per meeting? Even if it's only like $10, we should lump in that funding with the transcription costs.

[07:47:22.0958] <bakkot>
for actual Whisper it'll be free; it runs locally

[07:49:09.0748] <bakkot>
I could maybe cut a few seconds of lag off by using the API which would cost ~$6/meeting (somehow the API manages to be substantially faster than running locally), but the difference between 35 seconds and 40 seconds probably isn't worth worrying about

[10:11:18.0457] <shu>
if the model is in fact pretty much perfect in terms of accuracy, why not record the meeting and postprocess for transcription? then delete the recording afterwards

[10:12:33.0097] <Michael Ficarra>
shu: some people like to edit the notes immediately after speaking

[10:12:56.0149] <shu>
but is that because the accuracy is in doubt?

[10:13:36.0028] <Michael Ficarra>
I think some people make minor rephrasings, remove stumbles, etc

[10:13:57.0511] <shu>
fair enough

[10:22:25.0010] <Michael Ficarra>
personally, if the transcription is very accurate, I would be fine waiting until the end of the day (or week) to do my reviews

[10:23:31.0363] <Michael Ficarra>
but having the two docs sounds like a great compromise

[10:25:05.0324] <Michael Ficarra>
the worst part about reviewing notes for me is when the notes are either incomprehensible (our previous automatic transcription) or missing entire sentences (human transcription) and I can't remember what was said

[10:25:27.0737] <Michael Ficarra>
having a more accurate document to refer to would be so helpful for that

[10:38:32.0085] <bakkot>
the transcripts are also missing paragraph breaks and speaker assignments, and you really want to do those in real time

[10:40:03.0578] <bakkot>
 * the computer-generated transcripts are also missing paragraph breaks and speaker assignments, and you really want to do those in real time

[11:02:35.0177] <Ashley Claymore>
Another thing that we try and edit in live are when people post code snippets into TCQ or matrix. As the verbatim transcription of only the audio without the code can be almost meaningless 

[11:40:43.0124] <Michael Ficarra>
oh yeah, speaker attribution is actually pretty tricky to do after the fact

[12:29:30.0025] <Ashley Claymore>
Simply get everyone to commit to saying their acronym at the start of each sentence 

[12:29:56.0361] <Ashley Claymore>
ðŸ˜…


2023-11-09
[11:34:40.0186] <shu>
rbuckton: `using` declarations are supposed to be early errors at script toplevels, right? is the TS playground allowing that incorrectly?

[11:47:45.0787] <rbuckton>
Yes, it should be an early error. The playground is wrong. 

[12:04:29.0255] <shu>
thanks for confirming

[12:08:13.0820] <rbuckton>
IIRC we should report an error in that case, but I'm not sure what settings the playground uses offhand. If you send me a playground link, I'll try to look at it tomorrow as I'm traveling today


2023-11-10
[16:35:06.0286] <Michael Ficarra>
Feature-Sensitive Coverage for Conformance Testing of Programming Language Implementations: https://dl.acm.org/doi/pdf/10.1145/3591240

[16:35:16.0977] <Michael Ficarra>
> We extend JEST, the state-of-the-art JavaScript conformance test synthesizer using coverage-guided mutational fuzzing, with various FS and FCPS coverage criteria. For the latest JavaScript language specification (ES13, 2022), our tool automatically synthesizes 237,981 conformance tests in 50 hours with five coverage criteria. We evaluated the conformance of eight mainstream JavaScript implementations (four engines and four transpilers) with the synthesized conformance tests and discovered bugs in all of them. The tool detected 143 distinct conformance bugs (42 in engines and 101 in transpilers), 85 of which were confirmed by the developers and 83 of which were newly discovered bugs.


2023-11-11
[08:56:01.0380] <Mathieu Hofman>
> <@bakkot:matrix.org> the computer-generated transcripts are also missing paragraph breaks and speaker assignments, and you really want to do those in real time

We just need another AI to merge the human annotated transcript with the recording.

[09:05:29.0394] <bakkot>
I have actually seriously considered hooking up a special-purpose speaker diarization model; that's definitely a thing

[09:06:32.0754] <bakkot>
but the whole system is a bit of a mess already


2023-11-13
[09:21:50.0269] <Rob Palmer>
The Interest Survey for the 100th TC39 meeting in February in San Diego is posted ðŸŽ‰ 

-  https://github.com/tc39/Reflector/issues/512

Please add yourself by Tuesday 21st November.

[09:23:53.0218] <shu>
is there more info on where in SD?

[09:24:07.0888] <shu>
ah it's on the top of the sheet, thanks

[09:32:58.0216] <Rob Palmer>
I am intrigued as to whether the precise part of SD makes a difference to your choice.

[09:36:48.0497] <shu>
if timezone is not inconvenient to be remote, i personally do not want to go somewhere where either i'll have to rent a car or take taxis everyday to get to the venue, or have to stay, like, near some office park outside of the city

[09:38:56.0105] <Rob Palmer>
ah I see

[10:45:25.0271] <Michael Ficarra>
Same, bike share or walkability is super important for me. I will not rent a car, and would prefer not to take 5-minute Ubers multiple times a day. SD doesn't appear to have a bike share program, so I may end up trying out those electric scooters for the first time.

[10:47:20.0211] <ptomato>
how navigable is San Diego without a car in general? never been there

[10:59:04.0516] <shu>
> <@michaelficarra:matrix.org> Same, bike share or walkability is super important for me. I will not rent a car, and would prefer not to take 5-minute Ubers multiple times a day. SD doesn't appear to have a bike share program, so I may end up trying out those electric scooters for the first time.

yeah i say "rent a car" in the general sense, i will also not rent a car

[11:07:03.0514] <ljharb>
i don't mind ubers but scooters/walkability is a huge plus

[11:07:08.0969] <ljharb>
 * i don't mind ubers but scooters/walkability is a huge plus. def not renting a car

[12:42:19.0125] <Rob Palmer>
Let's see if we can coordinate on hotels to maximise lift-sharing.

[12:57:26.0258] <ethanarrowood>
San Diego is real nice. itâ€™s more spread out than SF so depending on hotel<->venue location scooters may not be feasible


2023-11-14
[17:00:03.0117] <rkirsling>
interesting

[17:00:46.0495] <rkirsling>
I assumed rent-a-car would be unavoidable for SD but if multiple people are opposed it'd be sweet to find another solution

[17:00:52.0494] <rkirsling>
I too don't enjoy driving

[11:05:42.0782] <snek>
can only member companies host

[11:06:33.0583] <Chris de Almeida>
> <@devsnek:matrix.org> can only member companies host

no

[11:09:36.0580] <Chris de Almeida>
https://github.com/tc39/how-we-work/blob/main/host.md

[11:11:31.0917] <snek>
neat

[13:00:14.0835] <bakkot>
man, "give me all of the own enumerable string + symbol keys of this object" is a _surprisingly_ annoying task for something I want to do pretty often

[13:01:53.0008] <bakkot>
```
let desc = Object.getOwnPropertyDescriptors(obj);
return [...Object.getOwnPropertyNames(desc), ...Object.getOwnPropertySymbols(desc)]
  .filter(x => desc[x].enumerable);
```
is the best I've got

[13:02:54.0564] <bakkot>
do you figure we could add get away with making `Object.keys`/`entries` take a options bag argument like `{ symbols: true }`

[13:05:07.0271] <bakkot>
alternative I guess is a `Object.symbolKeys` / `Object.symbolEntries` so you could at least do `[...Object.keys(x), ...Object.symbolKeys(x)]`

[13:10:11.0499] <ljharb>
`Reflect.ownKeys` doesn't do it?

[13:10:18.0109] <ljharb>
 * `Reflect.ownKeys` + a filter doesn't do it?

[13:10:52.0770] <ljharb>
(also why would you use gOPN if you want only enumerable? that's what keys already gives you)

[13:22:48.0439] <littledan>
wasn't that sort of the point, that symbols are supposed to be pseudo-private so it should be annoying to reflect over them?

[13:22:49.0060] <Michael Ficarra>
ljharb: keys is strings only

[13:23:11.0224] <littledan>
or was that just a compat decision?

[13:23:21.0599] <Michael Ficarra>
```

[13:23:38.0671] <Michael Ficarra>
 *   let keys = Object.entries(Object.getOwnPropertyDescriptors(input))
    .filter(([, desc]) => desc.enumerable)
    .map(([k]) => k);

[13:23:57.0352] <Michael Ficarra>
 * ```
let keys = Object.entries(Object.getOwnPropertyDescriptors(input))
    .filter((\[, desc\]) => desc.enumerable)
    .map((\[k\]) => k);

[13:24:12.0270] <Michael Ficarra>
 * ```
let keys = Object.entries(Object.getOwnPropertyDescriptors(input))
    .filter(([, desc]) => desc.enumerable)
    .map(([k]) => k);
```

[14:16:05.0568] <bakkot>
> <@ljharb:matrix.org> `Reflect.ownKeys` + a filter doesn't do it?

that's `Reflect.ownKeys` an improvement on using `getOwnPropertyNames` + `getOwnPropertySymbols`, sure

[14:16:18.0697] <bakkot>
> <@ljharb:matrix.org> `Reflect.ownKeys` + a filter doesn't do it?

 * ah, `Reflect.ownKeys` is an improvement on using `getOwnPropertyNames` + `getOwnPropertySymbols`, sure

[14:16:39.0167] <bakkot>
> <@ljharb:matrix.org> (also why would you use gOPN if you want only enumerable? that's what keys already gives you)

keys doesn't include symbols, is the reason

[14:16:47.0435] <bakkot>
> <@ljharb:matrix.org> (also why would you use gOPN if you want only enumerable? that's what keys already gives you)

 * `Object.keys` doesn't include symbols, is the reason

[14:16:48.0037] <ljharb>
> <@michaelficarra:matrix.org> ljharb: keys is strings only

Object.keys is Object.getOwnPropertyNames, but only enumerables

[14:17:05.0153] <ljharb>
> <@bakkot:matrix.org> `Object.keys` doesn't include symbols, is the reason

right but you're using getOwnPropertySymbols for the symbols. gOPN's only use is if you want non-enumerable strings, which you don't

[14:17:24.0037] <bakkot>
sure, yes, that does end up being equivalent here

[15:16:52.0556] <Richard Gibson>
https://github.com/endojs/endo/blob/master/packages/pass-style/doc/copyRecord-guarantees.md#how-do-i-enumerate-thee-let-me-list-the-ways


2023-11-15
[08:19:41.0747] <Mathieu Hofman>
Yeah I was going to suggest `Reflect.ownKeys({...obj})`

[08:22:48.0615] <Mathieu Hofman>
 * Yeah I was going to suggest `Reflect.ownKeys({...obj})` but only if you don't mind accessors being triggered

[08:32:28.0617] <ljharb>
why do you need the spread (which is what triggers the accessors)?

[08:51:40.0746] <bakkot>
presumably to get only the enumerable properties

[13:03:13.0918] <ptomato>
let's say I wanted to rebase and resuscitate https://github.com/tc39/ecma262/pull/1498 - should I try to get access to push to that branch, or start a new PR?

[13:32:45.0054] <Chris de Almeida>
> <@pchimento:igalia.com> let's say I wanted to rebase and resuscitate https://github.com/tc39/ecma262/pull/1498 - should I try to get access to push to that branch, or start a new PR?

I'm guessing you'd rather not have to fork and submit PRs to that branch from your fork?

[13:36:34.0876] <ptomato>
I could do that as well, but since the PR has become quite different by rebasing it on latest main, that might well be more complicated

[13:36:43.0460] <Chris de Almeida>
ah I didn't notice it was only one commit

[13:41:41.0651] <Chris de Almeida>
perhaps I am missing something but isn't the rebase the same regardless of whether you have write access to the repo?

[13:51:47.0847] <ptomato>
I mean, once I've rebased, would you ("you" meaning anyone with an interest in this PR) prefer that I figure out how to push the result up to the same PR, so that the previous discussions can resume uninterrupted? or is it better to open a new PR and say it supersedes the old PR?

[13:56:50.0529] <Chris de Almeida>
the extra step is just you'd be pushing to a branch on your fork and creating a PR to merge that branch to the PR branch

I defer to the editors on this, as they are the ones with 1. write access to the repo and 2. can decide what they prefer re: previous discussions vs new PR

[13:57:10.0205] <Chris de Almeida>
 * the extra step is just you'd be pushing to a branch on your fork and creating a PR to merge that branch to the PR branch

I defer to the editors on this, as they are the ones with 1. write access to the repo and would be the ones to merge your PR and 2. can decide what they prefer re: previous discussions vs new PR

[13:57:40.0327] <Chris de Almeida>
 * the extra step is just you'd be pushing to a branch on your fork and creating a PR to merge that branch to the PR branch

I defer to the editors on this, as they are the folks with 1. write access to the repo and would be the folks to merge your PR and 2. can decide what they prefer re: previous discussions on existing PR vs closing in favor of new PR

[13:58:03.0913] <Chris de Almeida>
 * the extra step is just you'd be pushing to a branch on your fork and creating a new PR to merge your fork branch to the existing PR branch

I defer to the editors on this, as they are the folks with 1. write access to the repo and would be the folks to merge your PR and 2. can decide what they prefer re: previous discussions on existing PR vs closing in favor of new PR

[14:24:58.0431] <Michael Ficarra>
I'm fine either way. A new PR would be less noisy, but it would be a shame to not ping the current thread participants about an update/replacement PR.


2023-11-16
[00:17:16.0573] <Rob Palmer>
A reminder:  The Interest Survey for the 100th TC39 meeting in February in San Diego is posted ðŸŽ‰

- https://github.com/tc39/Reflector/issues/512

Please add yourself by Tuesday 21st November.

[08:44:32.0442] <eemeli>
For those interested, I'm putting together an incubator call for the Stable Formatting proposal. See here for an agenda and a Doodle poll for figuring out the date & time: https://github.com/tc39/Reflector/issues/513

[14:24:02.0029] <bakkot>
from jmdyck:

```
Function(
  'a = console.log("oh"',
  '"no")',
  ''
)()
```

[14:25:36.0699] <bakkot>
this is very dumb, though I don't think it's an actual problem for anyone (in particular SES implements the spec as written, i.e., join with a comma and then validate as parameters)

[14:26:42.0581] <ptomato>
isn't that disallowed as described in the note in step 18 of https://tc39.es/ecma262/#sec-createdynamicfunction?

[14:27:24.0731] <ptomato>
oh, I guess the parameters as a whole are validated, not each parameter individually

[14:27:44.0831] <ptomato>
"oh no" indeed

[14:28:00.0957] <bakkot>
yup

[14:28:26.0114] <bakkot>
and people do rely on the ability to do `Function('a, b', 'body')` - combining multiple parameters into one argument, rather than putting each into their own argument


2023-11-17
[13:28:56.0963] <Chris de Almeida>
upcoming plenary on the calendar was mistakenly extended for 2 hours per day.  it has been corrected


2023-11-18
[07:55:56.0293] <Jack Works>
hello, is there any syntax pro? I'm stuck on designing cover grammar

[07:58:07.0854] <Jack Works>
Grammar:

```
MatchStatement: `match` [nLTh] `(` Expression `) [nLTh] `{ MatchStatementClauses `;` `}`
```

This is ambiguous with ExpressionStatement, therefore I need a CoverExpressionStatementAndMatchStatement grammar

[07:58:32.0718] <Jack Works>
 * Grammar:

```
MatchStatement: `match` [nLTh] `(` Expression `) [nLTh] `{ MatchStatementClauses `;` `}`
```

This is ambiguous with ExpressionStatement (cannot decide `match (expr)` until see `{` or any other token), therefore I need a CoverExpressionStatementAndMatchStatement grammar

[07:58:40.0540] <Jack Works>
 * Grammar:

```
MatchStatement: `match` [nLTh] `(` Expression `) [nLTh] `{ MatchStatementClauses `;` `}`
```

This is ambiguous with ExpressionStatement (cannot decide production for `match (expr)` until see `{` or any other token), therefore I need a CoverExpressionStatementAndMatchStatement grammar

[08:00:48.0809] <Jack Works>
Now I have this cover grammar:

```
CoverExpressionStatementAndMatchStatement :
    match [no LineTerminator here] Arguments 
```

This can cover the match head `match (a, b, c)` (where `,` is comma operator) and a call expression `match (a, b, c)` (where `,` is parameter separator). But I don't know how to use it in ExpressionStatement

[08:01:50.0684] <Jack Works>
```
MatchStatement :
    CoverExpressionStatementAndMatchStatement [no LineTerminator here] `{ ` MatchStatementClauses `;` `}`
```

This looks good to me for now

[08:03:39.0384] <Jack Works>
```
ExpressionStatement[Yield, Await] :
        CoverExpressionStatementAndMatchStatement
        (current definition)
```

This one does not. It does not cover everything that can follow a CallExpression, like `match (expr) + 1` or `match(expr).prop`.

[08:05:05.0982] <Jack Works>
It looks like I need to add all expressions to this cover grammar (e.g. it can follow a `++` or `.x` or `[prop]` or `(...args`)) which is unrealistic.

