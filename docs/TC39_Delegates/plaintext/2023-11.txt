2023-11-01
[18:00:02.0504] <snek>
i think torque could support suspend points natively with a desugaring

[18:00:27.0639] <snek>
this was one of the things i had in mind when i originally suggested the builtin async functions/generators

[18:40:29.0736] <shu>
by all means, i'd love someone to implement it

[18:40:40.0476] <shu>
 * by all means, i'd love for someone to implement it

[23:30:07.0012] <Ashley Claymore>
If anyone wants to give feedback ahead of the 'state of JS' survey: https://github.com/Devographics/surveys/issues/224 

[02:13:41.0650] <yulia>
How long has test262.report been down for? are there plans to bring it back up?

[03:54:00.0400] <nicolo-ribaudo>
test262.fyi :)

[03:55:17.0593] <yulia>
Aaaahhh

[08:47:06.0769] <bakkot>
base64 proposal is updated to remove streaming, fill out spec text, and settle outstanding questions about the API: https://github.com/tc39/proposal-arraybuffer-base64

see playground for overview and a polyfill in the console: https://tc39.es/proposal-arraybuffer-base64/

other than removing streaming, the most significant change is that the base64 decoder is now permissive by default (ignores whitespace, does not require padding), with a `strict: true` options bag argument which makes it strict (does not allow whitespace, enforces padding)

would appreciate eyes on it, and especially if anyone wants to volunteer to review before the November plenary so I can ask for stage 3 (assuming Peter no longer objects); otherwise I'll be asking in January

[09:09:19.0031] <ljharb>
I’ll volunteer to review


2023-11-02
[10:39:25.0273] <nicolo-ribaudo>
Now that we have dates & time zones for the next meetings, could we add them to the calendar?

[10:39:26.0580] <nicolo-ribaudo>
(or I can do it by myself but I don't know how to add events to the calendar)

[14:18:56.0001] <ljharb>
i'll make sure they're on there


2023-11-07
[23:04:55.0799] <bakkot>
openai announced a new Whisper model, but still no streaming support :(

all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.

whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).

I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)

I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.

anyone happen to have played with any other promising real-time transcription services recently?

[03:03:37.0883] <littledan>
> <@bakkot:matrix.org> openai announced a new Whisper model, but still no streaming support :(
> 
> all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.
> 
> whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).
> 
> I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)
> 
> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.
> 
> anyone happen to have played with any other promising real-time transcription services recently?

saminahusain: 

[03:04:04.0429] <littledan>
Thanks for the report, bakkot . Let's check up on this again at the end of next year.

[03:04:16.0687] <littledan>
sounds like we need to repeat the budget request for transcriptionists

[03:04:33.0931] <littledan>
Are you saying we get good accuracy with a 40-second delay?

[03:04:58.0007] <ryzokuken>
it would be accurate, yeah IIUC

[03:05:17.0188] <ryzokuken>
the 40 second delay is whisper's only shortcoming

[03:06:32.0966] <ryzokuken>
actually, I haven't tried it myself. Wonder how well it does with various accents

[03:07:44.0331] <ryzokuken>
they have an example with a pretty thick accent though, fun

[03:07:45.0834] <ryzokuken>
https://openai.com/research/whisper

[07:11:54.0820] <bakkot>
right. Whisper is very accurate in my tests, but it fundamentally operates on 30-second chunks of audio and takes a little while to run (say 10 seconds per chunk), so trying to stream it to the notes doc would mean that every 30 seconds we get a high-quality transcript of the portion of the meeting starting 40 seconds ago and running through 10 seconds ago. I haven't actually set that up but I expect it to work.

[07:12:19.0097] <bakkot>
unfortunately 40 seconds of lag is a lot of lag

[07:25:11.0927] <Michael Ficarra>
> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.

That would be *so* helpful!

