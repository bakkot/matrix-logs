2023-11-01
[18:00:02.0504] <snek>
i think torque could support suspend points natively with a desugaring

[18:00:27.0639] <snek>
this was one of the things i had in mind when i originally suggested the builtin async functions/generators

[18:40:29.0736] <shu>
by all means, i'd love someone to implement it

[18:40:40.0476] <shu>
 * by all means, i'd love for someone to implement it

[23:30:07.0012] <Ashley Claymore>
If anyone wants to give feedback ahead of the 'state of JS' survey: https://github.com/Devographics/surveys/issues/224 

[02:13:41.0650] <yulia>
How long has test262.report been down for? are there plans to bring it back up?

[03:54:00.0400] <nicolo-ribaudo>
test262.fyi :)

[03:55:17.0593] <yulia>
Aaaahhh

[08:47:06.0769] <bakkot>
base64 proposal is updated to remove streaming, fill out spec text, and settle outstanding questions about the API: https://github.com/tc39/proposal-arraybuffer-base64

see playground for overview and a polyfill in the console: https://tc39.es/proposal-arraybuffer-base64/

other than removing streaming, the most significant change is that the base64 decoder is now permissive by default (ignores whitespace, does not require padding), with a `strict: true` options bag argument which makes it strict (does not allow whitespace, enforces padding)

would appreciate eyes on it, and especially if anyone wants to volunteer to review before the November plenary so I can ask for stage 3 (assuming Peter no longer objects); otherwise I'll be asking in January

[09:09:19.0031] <ljharb>
I’ll volunteer to review


2023-11-02
[10:39:25.0273] <nicolo-ribaudo>
Now that we have dates & time zones for the next meetings, could we add them to the calendar?

[10:39:26.0580] <nicolo-ribaudo>
(or I can do it by myself but I don't know how to add events to the calendar)

[14:18:56.0001] <ljharb>
i'll make sure they're on there


2023-11-07
[23:04:55.0799] <bakkot>
openai announced a new Whisper model, but still no streaming support :(

all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.

whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).

I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)

I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.

anyone happen to have played with any other promising real-time transcription services recently?

[03:03:37.0883] <littledan>
> <@bakkot:matrix.org> openai announced a new Whisper model, but still no streaming support :(
> 
> all the services which offer real-time transcription, including those which just wrap whisper with some hacks, are basically garbage relative to whisper. google has a new model (Chirp) this year, but it doesn't work with real-time transcription either.
> 
> whisper gets near-perfect transcriptions for content like our meetings, everyone else misses one word in five. but using whisper without any of the streaming hacks (which lower quality a lot) means transcriptions will necessarily be 30 seconds behind (+ time to transcribe and network latency, so in practice more like 40 seconds).
> 
> I don't think automatic transcription is going to be viable until something in this landscape changes. (cc littledan)
> 
> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.
> 
> anyone happen to have played with any other promising real-time transcription services recently?

saminahusain: 

[03:04:04.0429] <littledan>
Thanks for the report, bakkot . Let's check up on this again at the end of next year.

[03:04:16.0687] <littledan>
sounds like we need to repeat the budget request for transcriptionists

[03:04:33.0931] <littledan>
Are you saying we get good accuracy with a 40-second delay?

[03:04:58.0007] <ryzokuken>
it would be accurate, yeah IIUC

[03:05:17.0188] <ryzokuken>
the 40 second delay is whisper's only shortcoming

[03:06:32.0966] <ryzokuken>
actually, I haven't tried it myself. Wonder how well it does with various accents

[03:07:44.0331] <ryzokuken>
they have an example with a pretty thick accent though, fun

[03:07:45.0834] <ryzokuken>
https://openai.com/research/whisper

[07:11:54.0820] <bakkot>
right. Whisper is very accurate in my tests, but it fundamentally operates on 30-second chunks of audio and takes a little while to run (say 10 seconds per chunk), so trying to stream it to the notes doc would mean that every 30 seconds we get a high-quality transcript of the portion of the meeting starting 40 seconds ago and running through 10 seconds ago. I haven't actually set that up but I expect it to work.

[07:12:19.0097] <bakkot>
unfortunately 40 seconds of lag is a lot of lag

[07:25:11.0927] <Michael Ficarra>
> I might set up a separate 40-second-latency transcription notes doc at the next meeting to help with fixing things the human transcriptionists miss.

That would be *so* helpful!

[07:45:46.0531] <Michael Ficarra>
bakkot: What will this cost per meeting? Even if it's only like $10, we should lump in that funding with the transcription costs.

[07:47:22.0958] <bakkot>
for actual Whisper it'll be free; it runs locally

[07:49:09.0748] <bakkot>
I could maybe cut a few seconds of lag off by using the API which would cost ~$6/meeting (somehow the API manages to be substantially faster than running locally), but the difference between 35 seconds and 40 seconds probably isn't worth worrying about

[10:11:18.0457] <shu>
if the model is in fact pretty much perfect in terms of accuracy, why not record the meeting and postprocess for transcription? then delete the recording afterwards

[10:12:33.0097] <Michael Ficarra>
shu: some people like to edit the notes immediately after speaking

[10:12:56.0149] <shu>
but is that because the accuracy is in doubt?

[10:13:36.0028] <Michael Ficarra>
I think some people make minor rephrasings, remove stumbles, etc

[10:13:57.0511] <shu>
fair enough

[10:22:25.0010] <Michael Ficarra>
personally, if the transcription is very accurate, I would be fine waiting until the end of the day (or week) to do my reviews

[10:23:31.0363] <Michael Ficarra>
but having the two docs sounds like a great compromise

[10:25:05.0324] <Michael Ficarra>
the worst part about reviewing notes for me is when the notes are either incomprehensible (our previous automatic transcription) or missing entire sentences (human transcription) and I can't remember what was said

[10:25:27.0737] <Michael Ficarra>
having a more accurate document to refer to would be so helpful for that

[10:38:32.0085] <bakkot>
the transcripts are also missing paragraph breaks and speaker assignments, and you really want to do those in real time

[10:40:03.0578] <bakkot>
 * the computer-generated transcripts are also missing paragraph breaks and speaker assignments, and you really want to do those in real time

[11:02:35.0177] <Ashley Claymore>
Another thing that we try and edit in live are when people post code snippets into TCQ or matrix. As the verbatim transcription of only the audio without the code can be almost meaningless 

[11:40:43.0124] <Michael Ficarra>
oh yeah, speaker attribution is actually pretty tricky to do after the fact

